I0427 02:18:35.664319       1 feature_gate.go:190] feature gates: map[ServiceNodeExclusion:true]
I0427 02:18:35.664454       1 controllermanager.go:108] Version: v1.9.7
I0427 02:18:35.672692       1 leaderelection.go:174] attempting to acquire leader lease...
E0427 02:18:35.673012       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:39.124718       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:43.382466       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:46.977801       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:50.028820       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:53.048443       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:56.698477       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:18:58.856587       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:01.232764       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:03.465997       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:06.188661       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:09.444552       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:13.397725       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:15.912446       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:18.826539       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:21.590335       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:24.716074       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:27.395932       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:30.099886       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:33.730242       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:36.255375       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:38.743516       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
E0427 02:19:41.610148       1 leaderelection.go:224] error retrieving resource lock kube-system/kube-controller-manager: Get https://10.255.255.5:443/api/v1/namespaces/kube-system/endpoints/kube-controller-manager: dial tcp 10.255.255.5:443: getsockopt: connection refused
I0427 02:19:49.966219       1 leaderelection.go:184] successfully acquired lease kube-system/kube-controller-manager
I0427 02:19:49.969158       1 event.go:218] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"7670538d-49c1-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"16", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' k8s-master-33591117-0 became leader
E0427 02:19:49.978190       1 controllermanager.go:387] Server isn't healthy yet.  Waiting a little while.
E0427 02:19:50.985170       1 controllermanager.go:387] Server isn't healthy yet.  Waiting a little while.
E0427 02:19:51.984877       1 controllermanager.go:387] Server isn't healthy yet.  Waiting a little while.
I0427 02:19:53.018344       1 azure.go:249] azure: using client_id+client_secret to retrieve access token
W0427 02:19:54.475838       1 controllermanager.go:478] "tokencleaner" is disabled
I0427 02:19:54.475856       1 controllermanager.go:484] Starting "attachdetach"
I0427 02:19:54.475895       1 controller_utils.go:1019] Waiting for caches to sync for tokens controller
I0427 02:19:54.576079       1 controller_utils.go:1026] Caches are synced for tokens controller
W0427 02:19:54.617596       1 probe.go:215] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
I0427 02:19:54.621889       1 plugins.go:453] Loaded volume plugin "kubernetes.io/aws-ebs"
I0427 02:19:54.621946       1 plugins.go:453] Loaded volume plugin "kubernetes.io/gce-pd"
I0427 02:19:54.621976       1 plugins.go:453] Loaded volume plugin "kubernetes.io/cinder"
I0427 02:19:54.622043       1 plugins.go:453] Loaded volume plugin "kubernetes.io/portworx-volume"
I0427 02:19:54.622072       1 plugins.go:453] Loaded volume plugin "kubernetes.io/vsphere-volume"
I0427 02:19:54.622097       1 plugins.go:453] Loaded volume plugin "kubernetes.io/azure-disk"
I0427 02:19:54.622134       1 plugins.go:453] Loaded volume plugin "kubernetes.io/photon-pd"
I0427 02:19:54.622161       1 plugins.go:453] Loaded volume plugin "kubernetes.io/scaleio"
I0427 02:19:54.622197       1 plugins.go:453] Loaded volume plugin "kubernetes.io/storageos"
I0427 02:19:54.622232       1 plugins.go:453] Loaded volume plugin "kubernetes.io/fc"
I0427 02:19:54.622258       1 plugins.go:453] Loaded volume plugin "kubernetes.io/iscsi"
I0427 02:19:54.622282       1 plugins.go:453] Loaded volume plugin "kubernetes.io/rbd"
I0427 02:19:54.622539       1 controllermanager.go:494] Started "attachdetach"
I0427 02:19:54.622577       1 controllermanager.go:484] Starting "pvc-protection"
I0427 02:19:54.622794       1 attach_detach_controller.go:258] Starting attach detach controller
I0427 02:19:54.622843       1 controller_utils.go:1019] Waiting for caches to sync for attach detach controller
I0427 02:19:54.709344       1 controllermanager.go:494] Started "pvc-protection"
I0427 02:19:54.709473       1 pvc_protection_controller.go:101] Starting PVC protection controller
I0427 02:19:54.709495       1 controller_utils.go:1019] Waiting for caches to sync for PVC protection controller
I0427 02:19:54.709486       1 controllermanager.go:484] Starting "podgc"
I0427 02:19:54.752085       1 controllermanager.go:494] Started "podgc"
W0427 02:19:54.752107       1 controllermanager.go:478] "bootstrapsigner" is disabled
I0427 02:19:54.752115       1 controllermanager.go:484] Starting "ttl"
I0427 02:19:54.752210       1 gc_controller.go:76] Starting GC controller
I0427 02:19:54.752228       1 controller_utils.go:1019] Waiting for caches to sync for GC controller
I0427 02:19:54.801673       1 controllermanager.go:494] Started "ttl"
I0427 02:19:54.801957       1 controllermanager.go:484] Starting "route"
I0427 02:19:54.801987       1 core.go:133] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
W0427 02:19:54.802017       1 controllermanager.go:491] Skipping "route"
I0427 02:19:54.802055       1 controllermanager.go:484] Starting "persistentvolume-binder"
I0427 02:19:54.801803       1 ttl_controller.go:116] Starting TTL controller
I0427 02:19:54.802128       1 controller_utils.go:1019] Waiting for caches to sync for TTL controller
I0427 02:19:54.861616       1 plugins.go:453] Loaded volume plugin "kubernetes.io/host-path"
I0427 02:19:54.861641       1 plugins.go:453] Loaded volume plugin "kubernetes.io/nfs"
I0427 02:19:54.861662       1 plugins.go:453] Loaded volume plugin "kubernetes.io/glusterfs"
I0427 02:19:54.861671       1 plugins.go:453] Loaded volume plugin "kubernetes.io/rbd"
I0427 02:19:54.861686       1 plugins.go:453] Loaded volume plugin "kubernetes.io/quobyte"
I0427 02:19:54.861702       1 plugins.go:453] Loaded volume plugin "kubernetes.io/azure-file"
I0427 02:19:54.861710       1 plugins.go:453] Loaded volume plugin "kubernetes.io/flocker"
I0427 02:19:54.861726       1 plugins.go:453] Loaded volume plugin "kubernetes.io/portworx-volume"
I0427 02:19:54.861737       1 plugins.go:453] Loaded volume plugin "kubernetes.io/scaleio"
I0427 02:19:54.861767       1 plugins.go:453] Loaded volume plugin "kubernetes.io/local-volume"
I0427 02:19:54.861783       1 plugins.go:453] Loaded volume plugin "kubernetes.io/storageos"
I0427 02:19:54.861791       1 plugins.go:453] Loaded volume plugin "kubernetes.io/aws-ebs"
I0427 02:19:54.861800       1 plugins.go:453] Loaded volume plugin "kubernetes.io/gce-pd"
I0427 02:19:54.861817       1 plugins.go:453] Loaded volume plugin "kubernetes.io/cinder"
I0427 02:19:54.861825       1 plugins.go:453] Loaded volume plugin "kubernetes.io/vsphere-volume"
I0427 02:19:54.861839       1 plugins.go:453] Loaded volume plugin "kubernetes.io/azure-disk"
I0427 02:19:54.861848       1 plugins.go:453] Loaded volume plugin "kubernetes.io/photon-pd"
I0427 02:19:54.861881       1 controllermanager.go:494] Started "persistentvolume-binder"
I0427 02:19:54.861897       1 controllermanager.go:484] Starting "pv-protection"
I0427 02:19:54.861982       1 pv_controller_base.go:264] Starting persistent volume controller
I0427 02:19:54.861999       1 controller_utils.go:1019] Waiting for caches to sync for persistent volume controller
I0427 02:19:54.909339       1 controllermanager.go:494] Started "pv-protection"
I0427 02:19:54.909362       1 controllermanager.go:484] Starting "resourcequota"
I0427 02:19:54.909461       1 pv_protection_controller.go:83] Starting PV protection controller
I0427 02:19:54.909479       1 controller_utils.go:1019] Waiting for caches to sync for PV protection controller
I0427 02:19:55.002467       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {extensions ingresses}
I0427 02:19:55.002597       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {autoscaling horizontalpodautoscalers}
I0427 02:19:55.002755       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {extensions daemonsets}
I0427 02:19:55.002833       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {apps controllerrevisions}
I0427 02:19:55.002916       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {networking.k8s.io networkpolicies}
I0427 02:19:55.003070       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for { limitranges}
I0427 02:19:55.003154       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {rbac.authorization.k8s.io roles}
I0427 02:19:55.003244       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {policy poddisruptionbudgets}
I0427 02:19:55.003339       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {extensions deployments}
I0427 02:19:55.003408       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {apps replicasets}
I0427 02:19:55.003468       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {batch jobs}
I0427 02:19:55.003556       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {rbac.authorization.k8s.io rolebindings}
I0427 02:19:55.003680       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {apps statefulsets}
I0427 02:19:55.003755       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {events.k8s.io events}
I0427 02:19:55.003850       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for { podtemplates}
I0427 02:19:55.003929       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {extensions replicasets}
I0427 02:19:55.003997       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {apps daemonsets}
I0427 02:19:55.004072       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {apps deployments}
I0427 02:19:55.004149       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for {batch cronjobs}
I0427 02:19:55.004227       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for { endpoints}
I0427 02:19:55.004349       1 resource_quota_monitor.go:216] QuotaMonitor created object count evaluator for { serviceaccounts}
I0427 02:19:55.004428       1 controllermanager.go:494] Started "resourcequota"
I0427 02:19:55.004465       1 controllermanager.go:484] Starting "namespace"
I0427 02:19:55.004481       1 resource_quota_controller.go:272] Starting resource quota controller
I0427 02:19:55.004763       1 controller_utils.go:1019] Waiting for caches to sync for resource quota controller
I0427 02:19:55.004801       1 resource_quota_monitor.go:289] QuotaMonitor running
I0427 02:19:55.071365       1 controllermanager.go:494] Started "namespace"
I0427 02:19:55.071454       1 controllermanager.go:484] Starting "horizontalpodautoscaling"
I0427 02:19:55.071503       1 namespace_controller.go:186] Starting namespace controller
I0427 02:19:55.071523       1 controller_utils.go:1019] Waiting for caches to sync for namespace controller
I0427 02:19:55.681598       1 controllermanager.go:494] Started "horizontalpodautoscaling"
I0427 02:19:55.681623       1 controllermanager.go:484] Starting "daemonset"
I0427 02:19:55.681683       1 horizontal.go:128] Starting HPA controller
I0427 02:19:55.681699       1 controller_utils.go:1019] Waiting for caches to sync for HPA controller
I0427 02:19:55.924566       1 controllermanager.go:494] Started "daemonset"
I0427 02:19:55.928501       1 controllermanager.go:484] Starting "job"
I0427 02:19:55.924739       1 daemon_controller.go:232] Starting daemon sets controller
I0427 02:19:55.928869       1 controller_utils.go:1019] Waiting for caches to sync for daemon sets controller
I0427 02:19:56.063232       1 resource_quota_controller.go:434] syncing resource quota controller with updated resources from discovery: map[{ v1 podtemplates}:{} { v1 secrets}:{} {extensions v1beta1 ingresses}:{} {apps v1 deployments}:{} {events.k8s.io v1beta1 events}:{} { v1 replicationcontrollers}:{} { v1 resourcequotas}:{} { v1 persistentvolumeclaims}:{} { v1 serviceaccounts}:{} { v1 endpoints}:{} {extensions v1beta1 replicasets}:{} {apps v1 replicasets}:{} {apps v1 statefulsets}:{} {autoscaling v1 horizontalpodautoscalers}:{} {rbac.authorization.k8s.io v1 rolebindings}:{} { v1 configmaps}:{} {apps v1 daemonsets}:{} {batch v1beta1 cronjobs}:{} { v1 services}:{} {apps v1 controllerrevisions}:{} {extensions v1beta1 networkpolicies}:{} {extensions v1beta1 deployments}:{} {policy v1beta1 poddisruptionbudgets}:{} {rbac.authorization.k8s.io v1 roles}:{} { v1 events}:{} {extensions v1beta1 daemonsets}:{} {batch v1 jobs}:{} {networking.k8s.io v1 networkpolicies}:{} { v1 pods}:{} { v1 limitranges}:{}]
I0427 02:19:56.171783       1 controllermanager.go:494] Started "job"
I0427 02:19:56.171854       1 controllermanager.go:484] Starting "statefulset"
I0427 02:19:56.171942       1 job_controller.go:138] Starting job controller
I0427 02:19:56.171979       1 controller_utils.go:1019] Waiting for caches to sync for job controller
I0427 02:19:56.421385       1 controllermanager.go:494] Started "statefulset"
I0427 02:19:56.421455       1 controllermanager.go:484] Starting "csrcleaner"
I0427 02:19:56.421527       1 stateful_set.go:150] Starting stateful set controller
I0427 02:19:56.421545       1 controller_utils.go:1019] Waiting for caches to sync for stateful set controller
I0427 02:19:56.671748       1 controllermanager.go:494] Started "csrcleaner"
I0427 02:19:56.671781       1 controllermanager.go:484] Starting "csrsigning"
I0427 02:19:56.671871       1 cleaner.go:81] Starting CSR cleaner controller
I0427 02:19:56.820899       1 controllermanager.go:494] Started "csrsigning"
I0427 02:19:56.820961       1 controllermanager.go:484] Starting "service"
I0427 02:19:56.821122       1 certificate_controller.go:113] Starting certificate controller
I0427 02:19:56.821181       1 controller_utils.go:1019] Waiting for caches to sync for certificate controller
I0427 02:19:57.084348       1 controllermanager.go:494] Started "service"
I0427 02:19:57.084584       1 controllermanager.go:484] Starting "persistentvolume-expander"
W0427 02:19:57.084826       1 controllermanager.go:491] Skipping "persistentvolume-expander"
I0427 02:19:57.084981       1 controllermanager.go:484] Starting "endpoint"
I0427 02:19:57.084454       1 service_controller.go:187] Starting service controller
I0427 02:19:57.085289       1 controller_utils.go:1019] Waiting for caches to sync for service controller
I0427 02:19:57.322753       1 controllermanager.go:494] Started "endpoint"
I0427 02:19:57.322771       1 controllermanager.go:484] Starting "deployment"
I0427 02:19:57.322809       1 endpoints_controller.go:153] Starting endpoint controller
I0427 02:19:57.322817       1 controller_utils.go:1019] Waiting for caches to sync for endpoint controller
I0427 02:19:57.571410       1 controllermanager.go:494] Started "deployment"
I0427 02:19:57.571436       1 controllermanager.go:484] Starting "node"
I0427 02:19:57.571477       1 deployment_controller.go:153] Starting deployment controller
I0427 02:19:57.571502       1 controller_utils.go:1019] Waiting for caches to sync for deployment controller
I0427 02:19:57.822099       1 node_controller.go:246] Sending events to api server.
I0427 02:19:57.822236       1 taint_controller.go:158] Sending events to api server.
I0427 02:19:57.822342       1 controllermanager.go:494] Started "node"
I0427 02:19:57.822360       1 controllermanager.go:484] Starting "clusterrole-aggregation"
I0427 02:19:57.822399       1 node_controller.go:558] Starting node controller
I0427 02:19:57.822414       1 controller_utils.go:1019] Waiting for caches to sync for node controller
I0427 02:19:58.076345       1 controllermanager.go:494] Started "clusterrole-aggregation"
I0427 02:19:58.076370       1 controllermanager.go:484] Starting "replicationcontroller"
I0427 02:19:58.076423       1 clusterroleaggregation_controller.go:148] Starting ClusterRoleAggregator
I0427 02:19:58.076440       1 controller_utils.go:1019] Waiting for caches to sync for ClusterRoleAggregator controller
I0427 02:19:58.321737       1 controllermanager.go:494] Started "replicationcontroller"
I0427 02:19:58.321815       1 replica_set.go:183] Starting replicationcontroller controller
I0427 02:19:58.321835       1 controller_utils.go:1019] Waiting for caches to sync for ReplicationController controller
I0427 02:19:58.321825       1 controllermanager.go:484] Starting "replicaset"
I0427 02:19:58.576900       1 controllermanager.go:494] Started "replicaset"
I0427 02:19:58.576998       1 controllermanager.go:484] Starting "serviceaccount"
I0427 02:19:58.577114       1 replica_set.go:183] Starting replicaset controller
I0427 02:19:58.577241       1 controller_utils.go:1019] Waiting for caches to sync for ReplicaSet controller
I0427 02:19:58.821065       1 controllermanager.go:494] Started "serviceaccount"
I0427 02:19:58.821081       1 controllermanager.go:484] Starting "garbagecollector"
I0427 02:19:58.821226       1 serviceaccounts_controller.go:115] Starting service account controller
I0427 02:19:58.821250       1 controller_utils.go:1019] Waiting for caches to sync for service account controller
I0427 02:20:00.176717       1 controllermanager.go:494] Started "garbagecollector"
I0427 02:20:00.176820       1 controllermanager.go:484] Starting "csrapproving"
I0427 02:20:00.176804       1 garbagecollector.go:135] Starting garbage collector controller
I0427 02:20:00.177064       1 controller_utils.go:1019] Waiting for caches to sync for garbage collector controller
I0427 02:20:00.177131       1 graph_builder.go:323] GraphBuilder running
I0427 02:20:00.184938       1 controllermanager.go:494] Started "csrapproving"
I0427 02:20:00.184952       1 controllermanager.go:484] Starting "disruption"
I0427 02:20:00.185066       1 certificate_controller.go:113] Starting certificate controller
I0427 02:20:00.185074       1 controller_utils.go:1019] Waiting for caches to sync for certificate controller
I0427 02:20:00.231206       1 controllermanager.go:494] Started "disruption"
I0427 02:20:00.232379       1 controllermanager.go:484] Starting "cronjob"
I0427 02:20:00.231375       1 disruption.go:288] Starting disruption controller
I0427 02:20:00.232506       1 controller_utils.go:1019] Waiting for caches to sync for disruption controller
I0427 02:20:00.281033       1 controllermanager.go:494] Started "cronjob"
I0427 02:20:00.281357       1 controller_utils.go:1019] Waiting for caches to sync for resource quota controller
I0427 02:20:00.281481       1 cronjob_controller.go:103] Starting CronJob Manager
E0427 02:20:00.295284       1 actual_state_of_world.go:491] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true because nodeName="k8s-master-33591117-0"  does not exist
E0427 02:20:00.295749       1 actual_state_of_world.go:491] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true because nodeName="k8s-agentpool-33591117-0"  does not exist
I0427 02:20:00.310956       1 controller_utils.go:1026] Caches are synced for TTL controller
I0427 02:20:00.332081       1 controller_utils.go:1026] Caches are synced for certificate controller
I0427 02:20:00.348580       1 ttl_controller.go:271] Changed ttl annotation for node k8s-master-33591117-0 to 0 seconds
I0427 02:20:00.362132       1 ttl_controller.go:271] Changed ttl annotation for node k8s-agentpool-33591117-0 to 0 seconds
I0427 02:20:00.372084       1 controller_utils.go:1026] Caches are synced for namespace controller
I0427 02:20:00.377213       1 controller_utils.go:1026] Caches are synced for ClusterRoleAggregator controller
I0427 02:20:00.381932       1 controller_utils.go:1026] Caches are synced for HPA controller
I0427 02:20:00.385594       1 controller_utils.go:1026] Caches are synced for service controller
I0427 02:20:00.385637       1 service_controller.go:654] Detected change in list of current cluster nodes. New node set: [k8s-master-33591117-0 k8s-agentpool-33591117-0]
I0427 02:20:00.385672       1 service_controller.go:662] Successfully updated 0 out of 0 load balancers to direct traffic to the updated set of nodes
I0427 02:20:00.385687       1 controller_utils.go:1026] Caches are synced for certificate controller
I0427 02:20:00.421301       1 controller_utils.go:1026] Caches are synced for service account controller
E0427 02:20:00.497088       1 clusterroleaggregation_controller.go:180] admin failed with : Operation cannot be fulfilled on clusterroles.rbac.authorization.k8s.io "admin": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:00.560289       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0427 02:20:00.560372       1 service_controller.go:326] Not persisting unchanged LoadBalancerStatus for service default/kubernetes to registry.
I0427 02:20:00.562159       1 controller_utils.go:1026] Caches are synced for persistent volume controller
I0427 02:20:00.571605       1 controller_utils.go:1026] Caches are synced for deployment controller
I0427 02:20:00.572320       1 controller_utils.go:1026] Caches are synced for job controller
I0427 02:20:00.577408       1 controller_utils.go:1026] Caches are synced for ReplicaSet controller
I0427 02:20:00.609622       1 controller_utils.go:1026] Caches are synced for PVC protection controller
I0427 02:20:00.609656       1 controller_utils.go:1026] Caches are synced for PV protection controller
I0427 02:20:00.621745       1 controller_utils.go:1026] Caches are synced for stateful set controller
I0427 02:20:00.621990       1 controller_utils.go:1026] Caches are synced for ReplicationController controller
I0427 02:20:00.622710       1 controller_utils.go:1026] Caches are synced for node controller
I0427 02:20:00.622756       1 node_controller.go:633] Controller observed a new Node: "k8s-master-33591117-0"
I0427 02:20:00.622792       1 taint_controller.go:181] Starting NoExecuteTaintManager
I0427 02:20:00.622999       1 controller_utils.go:1026] Caches are synced for attach detach controller
I0427 02:20:00.623000       1 controller_utils.go:197] Recording Registered Node k8s-master-33591117-0 in Controller event message for node k8s-master-33591117-0
I0427 02:20:00.623177       1 node_controller.go:611] Initializing eviction metric for zone: westus2: :0
I0427 02:20:00.623235       1 node_controller.go:633] Controller observed a new Node: "k8s-agentpool-33591117-0"
I0427 02:20:00.623271       1 controller_utils.go:197] Recording Registered Node k8s-agentpool-33591117-0 in Controller event message for node k8s-agentpool-33591117-0
W0427 02:20:00.623346       1 node_controller.go:964] Missing timestamp for Node k8s-master-33591117-0. Assuming now as a timestamp.
W0427 02:20:00.623397       1 node_controller.go:964] Missing timestamp for Node k8s-agentpool-33591117-0. Assuming now as a timestamp.
I0427 02:20:00.623014       1 controller_utils.go:1026] Caches are synced for endpoint controller
I0427 02:20:00.623678       1 event.go:218] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master-33591117-0", UID:"7638c5e6-49c1-11e8-a78c-000d3a016870", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node k8s-master-33591117-0 event: Registered Node k8s-master-33591117-0 in Controller
I0427 02:20:00.624218       1 node_controller.go:880] Controller detected that zone westus2: :0 is now in state Normal.
I0427 02:20:00.624291       1 event.go:218] Event(v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-agentpool-33591117-0", UID:"765963db-49c1-11e8-a78c-000d3a016870", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node k8s-agentpool-33591117-0 event: Registered Node k8s-agentpool-33591117-0 in Controller
I0427 02:20:00.629907       1 controller_utils.go:1026] Caches are synced for daemon sets controller
I0427 02:20:00.632694       1 controller_utils.go:1026] Caches are synced for disruption controller
I0427 02:20:00.632706       1 disruption.go:296] Sending events to api server.
I0427 02:20:00.652388       1 controller_utils.go:1026] Caches are synced for GC controller
I0427 02:20:00.677299       1 controller_utils.go:1026] Caches are synced for garbage collector controller
I0427 02:20:00.677318       1 garbagecollector.go:144] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0427 02:20:00.681577       1 controller_utils.go:1026] Caches are synced for resource quota controller
I0427 02:20:00.704958       1 controller_utils.go:1026] Caches are synced for resource quota controller
I0427 02:20:01.472913       1 garbagecollector.go:190] syncing garbage collector with updated resources from discovery: map[{events.k8s.io v1beta1 events}:{} {autoscaling v1 horizontalpodautoscalers}:{} {rbac.authorization.k8s.io v1 clusterrolebindings}:{} { v1 serviceaccounts}:{} { v1 podtemplates}:{} {extensions v1beta1 replicasets}:{} {batch v1 jobs}:{} {admissionregistration.k8s.io v1beta1 validatingwebhookconfigurations}:{} { v1 limitranges}:{} { v1 services}:{} {extensions v1beta1 deployments}:{} {extensions v1beta1 ingresses}:{} {extensions v1beta1 networkpolicies}:{} {apps v1 replicasets}:{} {batch v1beta1 cronjobs}:{} {certificates.k8s.io v1beta1 certificatesigningrequests}:{} { v1 namespaces}:{} { v1 pods}:{} {apps v1 daemonsets}:{} {policy v1beta1 poddisruptionbudgets}:{} {rbac.authorization.k8s.io v1 rolebindings}:{} {admissionregistration.k8s.io v1beta1 mutatingwebhookconfigurations}:{} { v1 events}:{} { v1 persistentvolumes}:{} { v1 replicationcontrollers}:{} {extensions v1beta1 podsecuritypolicies}:{} {networking.k8s.io v1 networkpolicies}:{} {rbac.authorization.k8s.io v1 clusterroles}:{} {rbac.authorization.k8s.io v1 roles}:{} {storage.k8s.io v1 storageclasses}:{} { v1 secrets}:{} {apiregistration.k8s.io v1beta1 apiservices}:{} {apps v1 deployments}:{} {apps v1 controllerrevisions}:{} {apps v1 statefulsets}:{} {apiextensions.k8s.io v1beta1 customresourcedefinitions}:{} { v1 persistentvolumeclaims}:{} { v1 nodes}:{} {extensions v1beta1 daemonsets}:{} { v1 endpoints}:{} { v1 configmaps}:{} { v1 resourcequotas}:{}]
I0427 02:20:02.606021       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"heapster", UID:"7dfaed11-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"326", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set heapster-8595f98c4c to 1
I0427 02:20:02.606552       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.616352       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
I0427 02:20:02.616832       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"328", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
E0427 02:20:02.621122       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.621214       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.631576       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.631636       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.631691       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.631958       1 deployment_controller.go:485] Error syncing deployment kube-system/heapster: Operation cannot be fulfilled on deployments.extensions "heapster": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:02.634385       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.638995       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.639071       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.640017       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.641838       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.651683       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.651754       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.651792       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.691969       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.703386       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.703471       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.703532       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.728422       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/metrics-server-99b948fd5, need 1, creating 1
I0427 02:20:02.729782       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0427 02:20:02.729811       1 service_controller.go:326] Not persisting unchanged LoadBalancerStatus for service kube-system/metrics-server to registry.
I0427 02:20:02.729910       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"metrics-server", UID:"7e0bebdd-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"346", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set metrics-server-99b948fd5 to 1
I0427 02:20:02.737828       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"metrics-server-99b948fd5", UID:"7e0dcd08-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"350", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: metrics-server-99b948fd5-tt9lq
I0427 02:20:02.752184       1 deployment_controller.go:485] Error syncing deployment kube-system/metrics-server: Operation cannot be fulfilled on deployments.extensions "metrics-server": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:02.776973       1 controller_utils.go:1019] Waiting for caches to sync for garbage collector controller
I0427 02:20:02.787232       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.797157       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0427 02:20:02.803014       1 service_controller.go:326] Not persisting unchanged LoadBalancerStatus for service kube-system/tiller-deploy to registry.
I0427 02:20:02.803129       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/tiller-deploy-7779ffcf89, need 1, creating 1
I0427 02:20:02.804379       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.804440       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.805113       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.805169       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"tiller-deploy", UID:"7e16ebaa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"367", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set tiller-deploy-7779ffcf89 to 1
I0427 02:20:02.823869       1 deployment_controller.go:485] Error syncing deployment kube-system/tiller-deploy: Operation cannot be fulfilled on deployments.extensions "tiller-deploy": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:02.877424       1 controller_utils.go:1026] Caches are synced for garbage collector controller
I0427 02:20:02.877846       1 garbagecollector.go:227] synced garbage collector
I0427 02:20:02.964655       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:02.976142       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Warning' reason: 'FailedCreate' Error creating: pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:02.976251       1 replica_set.go:515] Slow-start failure. Skipping creation of 1 pods, decrementing expectations for ReplicaSet kube-system/heapster-8595f98c4c
E0427 02:20:02.976339       1 replica_set.go:451] Sync "kube-system/heapster-8595f98c4c" failed with pods "heapster-8595f98c4c-" is forbidden: error looking up service account kube-system/heapster: serviceaccount "heapster" not found
I0427 02:20:03.169569       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/kube-dns-v20-597689868c, need 2, creating 2
I0427 02:20:03.170938       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"kube-dns-v20", UID:"7e5138ba-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"390", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set kube-dns-v20-597689868c to 2
I0427 02:20:03.177485       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"kube-dns-v20-597689868c", UID:"7e51e45e-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"391", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-dns-v20-597689868c-kprwh
I0427 02:20:03.178444       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0427 02:20:03.179209       1 service_controller.go:326] Not persisting unchanged LoadBalancerStatus for service kube-system/kube-dns to registry.
I0427 02:20:03.186540       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"kube-dns-v20-597689868c", UID:"7e51e45e-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"391", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-dns-v20-597689868c-wgnbq
I0427 02:20:03.217505       1 deployment_controller.go:485] Error syncing deployment kube-system/kube-dns-v20: Operation cannot be fulfilled on deployments.extensions "kube-dns-v20": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:03.250658       1 deployment_controller.go:485] Error syncing deployment kube-system/kube-dns-v20: Operation cannot be fulfilled on deployments.extensions "kube-dns-v20": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:03.292244       1 event.go:218] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"7e60bf79-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"410", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-gbgnh
I0427 02:20:03.296600       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/heapster-8595f98c4c, need 1, creating 1
I0427 02:20:03.311970       1 event.go:218] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"kube-system", Name:"kube-proxy", UID:"7e60bf79-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"410", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kube-proxy-tvdh9
I0427 02:20:03.312530       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"heapster-8595f98c4c", UID:"7dfc42aa-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"332", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: heapster-8595f98c4c-8jsm8
I0427 02:20:03.367274       1 deployment_controller.go:485] Error syncing deployment kube-system/heapster: Operation cannot be fulfilled on deployments.extensions "heapster": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:03.450653       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"kube-system", Name:"kubernetes-dashboard", UID:"7e7c262c-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"440", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set kubernetes-dashboard-697d9d4f78 to 1
I0427 02:20:03.451150       1 replica_set.go:478] Too few replicas for ReplicaSet kube-system/kubernetes-dashboard-697d9d4f78, need 1, creating 1
I0427 02:20:03.455717       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0427 02:20:03.455773       1 service_controller.go:326] Not persisting unchanged LoadBalancerStatus for service kube-system/kubernetes-dashboard to registry.
I0427 02:20:03.474787       1 deployment_controller.go:485] Error syncing deployment kube-system/kubernetes-dashboard: Operation cannot be fulfilled on deployments.extensions "kubernetes-dashboard": the object has been modified; please apply your changes to the latest version and try again
I0427 02:20:03.476550       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"kubernetes-dashboard-697d9d4f78", UID:"7e7ca4ea-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"441", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: kubernetes-dashboard-697d9d4f78-6dgvf
I0427 02:20:03.817355       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"kube-system", Name:"tiller-deploy-7779ffcf89", UID:"7e194205-49c1-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"369", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: tiller-deploy-7779ffcf89-2662w
W0427 02:35:08.476751       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 02:47:59.519344       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 02:56:35.599423       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 03:06:24.666256       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 03:16:02.715092       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0427 03:30:13.905039       1 azure_blobDiskController.go:550] azureDisk - failed to identify a suitable account for new disk and will attempt to create new account
I0427 03:30:14.541871       1 azure_blobDiskController.go:483] azureDisk - Creating storage account ds4c1e71e849cb11e8b4640 type Standard_LRS 
I0427 03:30:32.417413       1 azure_blobDiskController.go:396] azureDisk - storage account:ds4c1e71e849cb11e8b4640 had no default container(vhds) and it was created 
I0427 03:30:32.438122       1 pv_controller.go:1462] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azuredisk"
I0427 03:30:32.439083       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azuredisk", UID:"4c1c92b5-49cb-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"6035", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870 using kubernetes.io/azure-disk
I0427 03:30:32.443167       1 pv_controller.go:754] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" entered phase "Bound"
I0427 03:30:32.443192       1 pv_controller.go:893] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" bound to claim "default/pvc-azuredisk"
I0427 03:30:32.458846       1 pv_controller.go:698] claim "default/pvc-azuredisk" entered phase "Bound"
I0427 03:31:04.239985       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0427 03:31:04.395507       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0427 03:31:24.036400       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0427 03:33:48.713350       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0427 03:33:48.720926       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0427 03:33:48.898103       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0427 03:34:08.210327       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0427 03:34:08.210367       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
W0427 03:41:50.779377       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 03:54:01.821129       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 04:00:41.860328       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 04:14:45.894512       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 04:47:21.948341       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 04:57:17.041768       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 05:06:45.122849       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 05:16:21.170940       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 05:31:01.201986       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 05:40:45.263987       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 05:55:11.330212       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 06:11:09.423852       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 06:27:16.515807       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 06:35:36.611828       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 06:45:03.716420       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 06:57:59.769285       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 07:10:06.800726       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 07:28:56.885689       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 07:38:02.939001       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 07:47:37.997897       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:03:12.021041       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:10:31.086668       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:19:46.120140       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:33:53.219284       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:41:51.255511       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 08:57:16.300522       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 09:12:07.363517       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 09:24:23.444116       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 09:31:16.518820       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 09:44:47.538694       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 09:59:09.598581       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 10:06:49.667271       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 10:21:33.714209       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 10:31:33.820528       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 10:48:37.855830       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 10:56:13.919087       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 11:05:32.930510       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 11:22:36.938619       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 11:35:04.008963       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 11:49:33.065551       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 11:59:00.128639       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 12:05:55.197527       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 12:15:23.263591       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 12:27:52.324997       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 12:42:11.335760       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 12:51:44.433536       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 13:04:51.499061       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 13:18:10.516046       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 13:27:23.545992       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 13:43:07.630950       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 13:50:50.722650       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 14:05:11.825404       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 14:21:55.886034       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 14:29:54.914345       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 14:47:25.960825       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 14:55:47.038079       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 15:09:13.122513       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 15:18:49.204414       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 15:25:49.230212       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 15:41:37.249257       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 15:52:30.356031       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 16:07:01.415982       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 16:22:55.487019       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 16:37:44.564134       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 16:53:12.624923       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 17:05:09.692033       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 17:20:21.740185       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 17:30:15.778646       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 17:42:19.857137       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 17:51:26.914330       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 18:06:33.925736       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 18:18:45.017102       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 18:25:53.057300       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 18:40:29.112527       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 18:52:56.138437       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 19:02:29.244369       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 19:11:18.273153       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 19:20:09.341147       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 19:34:51.404650       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 19:51:07.445200       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:08:54.513126       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:17:51.545819       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:25:15.619342       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:35:09.666420       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:48:16.740293       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 20:56:06.793298       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 21:10:31.874187       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 21:26:24.911608       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 21:36:22.943213       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 21:52:36.990758       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:04:06.050525       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:12:40.157676       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:20:46.175292       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:30:37.212217       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:45:39.275656       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 22:59:31.346631       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 23:07:34.381895       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 23:19:20.486318       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 23:27:37.568263       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 23:37:12.601807       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0427 23:45:55.621006       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 00:01:24.722485       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 00:12:14.762066       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 00:27:35.782006       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 00:43:28.818544       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 00:56:08.845193       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 01:14:15.926307       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 01:23:47.947265       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 01:30:56.962838       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 01:45:14.060054       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:03:45.100976       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:14:23.122726       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:24:09.149125       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:31:18.198055       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:40:40.275957       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 02:55:27.287368       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 03:05:05.383870       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 03:20:46.431482       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 03:35:28.495103       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 03:45:18.530563       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 03:54:48.626477       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 04:10:36.711957       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 04:25:01.810379       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 04:37:09.887469       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 04:52:22.896619       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 05:07:48.987000       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 05:22:31.042290       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 05:38:43.136745       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 05:47:19.228500       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 05:58:23.250355       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 06:07:57.258865       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 06:17:14.360966       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 06:32:33.455240       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 06:41:47.553761       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 06:55:11.612501       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 07:10:36.655083       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 07:24:07.739142       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 07:36:29.815518       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 07:48:08.837655       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 07:55:32.893332       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:07:55.992735       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:16:44.058758       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:28:16.104910       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:37:57.133118       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:46:17.228876       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 08:59:40.266877       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 09:05:47.309586       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 09:18:53.397448       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 09:25:59.429379       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 09:41:30.458589       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 09:55:03.521110       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 10:09:43.577449       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 10:17:20.653635       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 10:27:07.727162       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 10:39:06.773716       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 10:47:53.864591       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 11:01:46.879773       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 11:11:34.905266       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 11:26:42.984637       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 11:36:39.015539       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 11:46:18.062822       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 12:03:02.117300       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 12:11:55.177398       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 12:20:15.255761       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 12:34:55.286616       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 12:51:49.383957       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 13:04:43.483768       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 13:17:10.500181       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 13:33:08.534936       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 13:40:46.584762       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 13:52:59.600984       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:02:46.642138       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:13:42.734562       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:23:36.796328       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:38:01.869000       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:47:27.962731       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 14:58:45.059602       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 15:08:24.094692       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 15:22:30.175971       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 15:30:00.245667       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0428 15:37:39.984610       1 pv_controller.go:1462] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azurefile"
I0428 15:37:39.984801       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azurefile", UID:"152724da-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"170740", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-152724da-4afa-11e8-a78c-000d3a016870 using kubernetes.io/azure-file
I0428 15:37:39.989356       1 pv_controller.go:754] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" entered phase "Bound"
I0428 15:37:39.989386       1 pv_controller.go:893] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" bound to claim "default/pvc-azurefile"
I0428 15:37:39.997435       1 pv_controller.go:698] claim "default/pvc-azurefile" entered phase "Bound"
I0428 15:42:12.441577       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0428 15:42:12.441656       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0428 15:42:12.442515       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-6f8749794b, need 1, creating 1
I0428 15:42:12.442529       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0428 15:42:12.457742       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"b8015ed4-4afa-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"171106", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-6f8749794b to 1
I0428 15:42:12.479158       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6f8749794b", UID:"b8049b90-4afa-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"171110", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-6f8749794b-24x9f
I0428 15:42:12.487260       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0428 15:42:12.543980       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0428 15:42:12.672214       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:42:12.672453       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0428 15:42:12.672574       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0428 15:42:12.775665       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0428 15:42:12.809136       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0428 15:42:16.561052       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0428 15:42:16.684000       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(true): started
I0428 15:42:16.777175       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:42:16.777213       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0428 15:42:16.777230       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0428 15:42:17.113086       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0428 15:42:17.113134       1 azure_backoff.go:58] backoff: success
I0428 15:42:17.757223       1 azure_backoff.go:58] backoff: success
I0428 15:42:32.447194       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0428 15:42:52.529111       1 replica_set.go:349] ReplicaSet "nginx-server-6f8749794b" will be enqueued after 5s for availability check
I0428 15:44:48.010074       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197) finished
I0428 15:44:58.322070       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0428 15:44:58.322473       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0428 15:45:46.851462       1 service_controller.go:763] Service has been deleted default/nginx-server
I0428 15:45:46.851840       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0428 15:45:46.901153       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:45:47.933193       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"b8015ed4-4afa-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"171415", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-6f8749794b to 0
I0428 15:45:47.933555       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-6f8749794b, need 0, deleting 1
I0428 15:45:47.933594       1 controller_utils.go:590] Controller nginx-server-6f8749794b deleting pod default/nginx-server-6f8749794b-24x9f
I0428 15:45:47.940939       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0428 15:45:47.953952       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6f8749794b", UID:"b8049b90-4afa-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"171417", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-6f8749794b-24x9f
I0428 15:45:50.973668       1 deployment_controller.go:574] Deployment default/nginx-server has been deleted
I0428 15:45:55.374516       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0428 15:45:55.383192       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0428 15:45:55.388916       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0428 15:45:57.196308       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0428 15:45:57.196397       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0428 15:45:57.229714       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:45:57.272164       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(false) resolved load balancer name
E0428 15:45:57.399708       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=429, err=network.LoadBalancersClient#Delete: Failure responding to request: StatusCode=429 -- Original Error: autorest/azure: Service returned an error. Status=429 Code="RetryableError" Message="A retryable error occurred." Details=[{"code":"ReferencedResourceNotProvisioned","message":"Cannot proceed with operation because resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Compute/availabilitySets/agentpool-availabilitySet-33591117 used by resource andy-k8s197 is not in Succeeded state. Resource is in Updating state and the last operation that updated/is updating the resource is InternalOperation."}]
I0428 15:45:57.399909       1 azure_loadbalancer.go:751] delete(default/nginx-server) abort backoff: lb(andy-k8s197) - deleting; no remaining frontendipconfigs
E0428 15:45:57.400040       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5s: timed out waiting for the condition
I0428 15:45:57.400170       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Warning' reason: 'DeletingLoadBalancerFailed' Error deleting load balancer (will retry): timed out waiting for the condition
I0428 15:46:02.400373       1 service_controller.go:763] Service has been deleted default/nginx-server
I0428 15:46:02.401009       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0428 15:46:02.432443       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:46:02.577909       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0428 15:46:02.608406       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0428 15:46:02.629112       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0428 15:46:21.868113       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0428 15:46:21.868167       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0428 15:48:43.743485       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0428 15:48:43.743599       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197) finished
I0428 15:48:43.917476       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0428 15:48:43.917506       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-ab80485bf4afa11e8a78c000d3a01687) - deleting
I0428 15:48:54.218182       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0428 15:48:54.218204       1 azure_loadbalancer.go:1192] ensure(default/nginx-server): pip(andy-k8s197-ab80485bf4afa11e8a78c000d3a01687) - finished
I0428 15:48:54.218213       1 azure_loadbalancer.go:165] delete(default/nginx-server): FINISH
I0428 15:48:54.218251       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"b80485bf-4afa-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"171109", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
W0428 16:00:51.274282       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 16:13:07.299580       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 16:29:02.382149       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 16:35:53.468704       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 16:58:17.508114       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 17:12:18.517901       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 17:27:14.579163       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 17:37:06.686417       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 17:50:52.792240       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 18:05:43.885835       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 18:21:03.927269       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 18:35:22.006676       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 18:45:05.026362       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 18:59:20.103368       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 19:07:30.144692       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 19:20:36.174379       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 19:34:21.263021       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 19:43:16.303266       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 19:50:18.382813       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 20:00:15.418712       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 20:15:07.449436       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 20:31:39.548182       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 20:40:37.576415       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 20:50:19.585655       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 21:06:17.653128       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 21:20:26.748296       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 21:37:52.818195       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 21:45:33.896203       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 21:55:04.970200       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 22:09:43.044768       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 22:16:59.107076       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 22:31:17.132737       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 22:40:49.232880       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 22:50:49.331233       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 23:04:08.404016       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 23:12:52.435708       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 23:23:22.483232       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 23:31:47.499895       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0428 23:46:19.560486       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 00:01:22.613102       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 00:17:11.638279       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 00:28:36.709658       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 00:36:50.786139       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 00:50:04.855890       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 01:05:24.924462       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 01:23:34.975611       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 01:32:44.060185       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 01:41:06.147310       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 01:50:29.255959       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 02:04:47.288482       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 02:14:39.366181       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 02:20:39.418283       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 02:35:47.462000       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 02:53:14.502448       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 03:01:16.608126       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 03:10:11.673270       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 03:24:58.733167       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 03:36:50.765620       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 03:52:28.872297       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:00:14.922511       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:17:52.985989       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:27:52.017636       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:35:03.088067       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:46:09.105576       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 04:59:40.125692       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 05:06:59.147354       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 05:21:21.257631       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 05:35:34.291150       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 05:52:44.327417       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 06:09:08.407901       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 06:18:49.421303       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 06:32:51.508595       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 06:40:35.534855       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 06:55:47.590248       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 07:11:16.689890       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 07:20:49.736586       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 07:35:08.766265       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 07:47:03.805300       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:03:26.825268       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:12:23.912849       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:21:05.983531       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:36:58.028485       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:46:53.072599       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 08:55:00.162600       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 09:06:09.192357       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 09:21:35.247714       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 09:33:09.331260       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 09:40:34.423507       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 09:56:23.498851       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 10:05:54.604854       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 10:21:16.637961       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 10:34:04.668144       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 10:42:24.728797       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 10:57:41.755928       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 11:06:52.776495       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 11:24:11.787027       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 11:33:32.848529       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 11:42:39.899539       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 11:51:17.991025       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 12:00:13.002074       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 12:18:12.111720       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 12:28:06.199287       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 12:40:41.304911       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 12:57:47.385796       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 13:05:02.394959       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 13:19:21.436475       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 13:25:13.532010       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 13:36:16.598926       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 13:51:16.698423       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 14:08:08.758698       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 14:23:21.776414       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 14:31:24.878075       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 14:45:41.897709       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 15:03:35.983603       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 15:12:05.002150       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 15:26:51.032971       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 15:35:17.068488       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 15:46:58.142383       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:01:23.224239       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:19:30.274245       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:27:05.311787       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:35:16.416913       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:49:26.510644       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 16:54:55.517073       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 17:09:10.587176       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 17:16:41.628660       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 17:25:23.681587       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 17:35:03.764327       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 17:49:58.836374       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 18:07:32.912278       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 18:22:25.976662       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 18:34:11.078604       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 18:43:28.138550       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 18:52:20.242659       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 19:04:41.321892       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 19:11:22.377388       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 19:25:45.390311       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 19:40:12.464347       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 19:57:09.503717       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 20:05:15.519063       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 20:19:52.533468       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 20:35:55.541230       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 20:45:38.627298       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 21:02:00.636105       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 21:10:48.727130       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 21:26:56.821393       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 21:39:39.868190       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 21:45:06.930476       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 22:01:14.974456       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 22:15:41.006586       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 22:32:15.024785       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 22:48:28.074982       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 22:54:58.132666       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 23:05:34.184224       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 23:22:48.288310       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 23:36:09.318022       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 23:45:33.392886       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0429 23:58:44.482068       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 00:06:57.584358       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 00:16:17.600324       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 00:30:00.660932       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 00:44:57.739967       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 00:59:37.842761       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 01:08:22.852365       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 01:17:10.941159       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 01:33:15.002433       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 01:40:23.028750       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 01:57:19.134311       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 02:05:43.221017       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 02:16:08.280851       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 02:25:06.344841       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 02:38:13.396801       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 02:46:36.437422       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 03:02:11.469579       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 03:16:52.541882       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 03:25:02.636388       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 03:39:48.721553       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 03:48:29.787109       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 04:01:09.885408       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 04:18:18.947186       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 04:25:23.972890       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 04:42:13.995141       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 04:54:20.013032       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 05:03:52.072311       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 05:11:28.133086       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 05:23:19.209572       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 05:30:51.253718       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 05:44:55.353082       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 06:04:01.429835       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 06:12:38.500454       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 06:26:43.537082       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 06:35:06.582478       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 06:50:58.653784       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:04:04.739711       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:10:47.820958       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:20:18.906701       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:33:00.943939       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:42:06.953753       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 07:51:03.035019       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 08:05:04.083692       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 08:22:16.132454       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 08:37:34.227244       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 08:47:45.287092       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 08:59:13.388473       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 09:05:35.520595       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 09:18:39.532296       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 09:25:48.635318       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 09:40:45.723789       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 09:52:56.822574       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:00:15.911617       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:14:47.940703       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:23:15.000923       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:32:11.083331       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:43:33.148171       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 10:57:20.226537       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 11:06:17.317417       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 11:22:48.329137       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 11:32:08.368140       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 11:44:50.440003       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 11:56:39.518688       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 12:12:50.575170       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 12:22:50.668815       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 12:30:49.684464       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 12:43:12.713804       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 12:57:10.812754       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 13:11:16.856116       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 13:28:15.875440       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 13:35:00.911758       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 13:44:58.961014       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 14:00:29.972097       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 14:15:49.015978       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 14:32:09.119746       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 14:45:03.224758       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0430 14:54:34.008862       1 pv_controller.go:542] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0430 14:54:34.023815       1 pv_controller.go:754] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" entered phase "Released"
I0430 14:54:34.026408       1 pv_controller.go:1220] isVolumeReleased[pvc-152724da-4afa-11e8-a78c-000d3a016870]: volume is released
I0430 14:54:34.082784       1 pv_controller.go:1259] volume "pvc-152724da-4afa-11e8-a78c-000d3a016870" deleted
I0430 14:54:34.097933       1 pv_controller_base.go:376] deletion of claim "default/pvc-azurefile" was already processed
I0430 14:54:38.492704       1 pv_controller.go:542] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0430 14:54:38.496046       1 pv_controller.go:754] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" entered phase "Released"
I0430 14:54:38.503422       1 pv_controller.go:1220] isVolumeReleased[pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870]: volume is released
I0430 14:54:38.570094       1 pv_controller.go:1259] volume "pvc-4c1c92b5-49cb-11e8-a78c-000d3a016870" deleted
I0430 14:54:38.574477       1 pv_controller_base.go:376] deletion of claim "default/pvc-azuredisk" was already processed
W0430 15:03:11.238771       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0430 15:07:41.946560       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-storage-demo-579fb57b45, need 1, creating 1
I0430 15:07:41.954844       1 service_controller.go:301] Ensuring LB for service default/nginx-storage-demo
I0430 15:07:41.955104       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-storage-demo) - wantLb(false): started
I0430 15:07:41.955756       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-storage-demo", UID:"3ab7bb34-4c88-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"387429", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-storage-demo-579fb57b45 to 1
I0430 15:07:41.956191       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0430 15:07:41.979529       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-storage-demo-579fb57b45", UID:"3aba9622-4c88-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"387433", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-storage-demo-579fb57b45-2pcrn
I0430 15:07:41.989477       1 azure_blobDiskController.go:537] azureDisk - account ds4c1e71e849cb11e8b4640 identified for a new disk  is because it has 0 allocated disks
I0430 15:07:41.997575       1 deployment_controller.go:485] Error syncing deployment default/nginx-storage-demo: Operation cannot be fulfilled on deployments.extensions "nginx-storage-demo": the object has been modified; please apply your changes to the latest version and try again
I0430 15:07:42.018834       1 deployment_controller.go:485] Error syncing deployment default/nginx-storage-demo: Operation cannot be fulfilled on deployments.extensions "nginx-storage-demo": the object has been modified; please apply your changes to the latest version and try again
I0430 15:07:42.045897       1 pv_controller.go:1462] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azuredisk"
I0430 15:07:42.046253       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azuredisk", UID:"3ab4281c-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387426", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-3ab4281c-4c88-11e8-a78c-000d3a016870 using kubernetes.io/azure-disk
I0430 15:07:42.054057       1 pv_controller.go:754] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" entered phase "Bound"
I0430 15:07:42.054092       1 pv_controller.go:893] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" bound to claim "default/pvc-azuredisk"
I0430 15:07:42.065254       1 pv_controller.go:698] claim "default/pvc-azuredisk" entered phase "Bound"
I0430 15:07:42.538795       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:07:42.538823       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-storage-demo): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0430 15:07:42.538843       1 azure_loadbalancer.go:800] ensure(default/nginx-storage-demo): lb(andy-k8s197-internal) finished
I0430 15:07:42.648437       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:07:42.730203       1 pv_controller.go:1462] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azurefile"
I0430 15:07:42.730306       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azurefile", UID:"3ab5e198-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387428", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-3ab5e198-4c88-11e8-a78c-000d3a016870 using kubernetes.io/azure-file
I0430 15:07:42.739343       1 pv_controller.go:754] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" entered phase "Bound"
I0430 15:07:42.739416       1 pv_controller.go:893] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" bound to claim "default/pvc-azurefile"
I0430 15:07:42.746785       1 pv_controller.go:698] claim "default/pvc-azurefile" entered phase "Bound"
I0430 15:07:43.063827       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:07:43.231494       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0430 15:07:46.725634       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:07:46.851651       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-storage-demo) - wantLb(true): started
I0430 15:07:46.932151       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:07:46.932183       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-storage-demo) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0430 15:07:46.932198       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-storage-demo): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0430 15:07:47.598485       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0430 15:07:47.598552       1 azure_backoff.go:58] backoff: success
I0430 15:07:47.762891       1 azure_backoff.go:58] backoff: success
E0430 15:07:48.049888       1 service_controller.go:776] Failed to process service default/nginx-storage-demo. Retrying in 5s: failed to ensure load balancer for service default/nginx-storage-demo: ensure(default/nginx-storage-demo): lb(andy-k8s197) - failed to ensure host in pool: "network.InterfacesClient#CreateOrUpdate: Failure responding to request: StatusCode=429 -- Original Error: autorest/azure: Service returned an error. Status=429 Code=\"RetryableError\" Message=\"A retryable error occurred.\" Details=[{\"code\":\"ReferencedResourceNotProvisioned\",\"message\":\"Cannot proceed with operation because resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Compute/availabilitySets/agentpool-availabilitySet-33591117 used by resource k8s-agentpool-33591117-nic-0 is not in Succeeded state. Resource is in Updating state and the last operation that updated/is updating the resource is InternalOperation.\"}]"
I0430 15:07:48.050040       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-storage-demo: ensure(default/nginx-storage-demo): lb(andy-k8s197) - failed to ensure host in pool: "network.InterfacesClient#CreateOrUpdate: Failure responding to request: StatusCode=429 -- Original Error: autorest/azure: Service returned an error. Status=429 Code=\"RetryableError\" Message=\"A retryable error occurred.\" Details=[{\"code\":\"ReferencedResourceNotProvisioned\",\"message\":\"Cannot proceed with operation because resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Compute/availabilitySets/agentpool-availabilitySet-33591117 used by resource k8s-agentpool-33591117-nic-0 is not in Succeeded state. Resource is in Updating state and the last operation that updated/is updating the resource is InternalOperation.\"}]"
I0430 15:07:53.051197       1 service_controller.go:301] Ensuring LB for service default/nginx-storage-demo
I0430 15:07:53.051289       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-storage-demo) - wantLb(false): started
I0430 15:07:53.051436       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0430 15:07:53.085944       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:07:53.085976       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-storage-demo): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0430 15:07:53.085996       1 azure_loadbalancer.go:800] ensure(default/nginx-storage-demo): lb(andy-k8s197-internal) finished
I0430 15:07:53.115059       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:07:53.134891       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-storage-demo) - wantLb(true): started
I0430 15:07:53.162875       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:07:53.191079       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-storage-demo): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0430 15:07:53.191161       1 azure_backoff.go:58] backoff: success
I0430 15:07:53.191300       1 azure_backoff.go:58] backoff: success
I0430 15:08:09.777577       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:08:38.060444       1 replica_set.go:349] ReplicaSet "nginx-storage-demo-579fb57b45" will be enqueued after 5s for availability check
I0430 15:10:14.268098       1 azure_loadbalancer.go:800] ensure(default/nginx-storage-demo): lb(andy-k8s197) finished
I0430 15:10:25.120399       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:10:25.120797       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0430 15:14:50.115306       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-storage-demo-579fb57b45, need 0, deleting 1
I0430 15:14:50.115341       1 controller_utils.go:590] Controller nginx-storage-demo-579fb57b45 deleting pod default/nginx-storage-demo-579fb57b45-2pcrn
I0430 15:14:50.116799       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-storage-demo", UID:"3ab7bb34-4c88-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388054", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-storage-demo-579fb57b45 to 0
I0430 15:14:50.126458       1 deployment_controller.go:485] Error syncing deployment default/nginx-storage-demo: Operation cannot be fulfilled on deployments.extensions "nginx-storage-demo": the object has been modified; please apply your changes to the latest version and try again
I0430 15:14:50.136649       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-storage-demo-579fb57b45", UID:"3aba9622-4c88-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388056", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-storage-demo-579fb57b45-2pcrn
I0430 15:14:53.152795       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-storage-demo-579fb57b45-2pcrn, uid: 3abeb11d-4c88-11e8-a78c-000d3a016870]
I0430 15:14:53.158342       1 deployment_controller.go:574] Deployment default/nginx-storage-demo has been deleted
I0430 15:14:53.897887       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:14:53.904166       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:14:53.910417       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0430 15:15:01.382603       1 service_controller.go:763] Service has been deleted default/nginx-storage-demo
I0430 15:15:01.384914       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0430 15:15:01.562601       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:15:09.858445       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"45b4257c-4c89-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388099", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-784cc9d865 to 1
I0430 15:15:09.858709       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-784cc9d865, need 1, creating 1
I0430 15:15:09.882500       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0430 15:15:09.884961       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-784cc9d865", UID:"45b54cd7-4c89-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388100", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-784cc9d865-7fpm6
I0430 15:15:12.030350       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:15:12.030378       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-storage-demo) - wantLb(false): started
I0430 15:15:12.117528       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:15:12.238838       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-storage-demo): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0430 15:15:13.627415       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0430 15:15:13.627543       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:15:13.659081       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:15:13.709811       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0430 15:17:42.963232       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:17:42.963347       1 azure_loadbalancer.go:800] ensure(default/nginx-storage-demo): lb(andy-k8s197) finished
I0430 15:17:43.076357       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:17:43.076386       1 azure_loadbalancer.go:1177] ensure(default/nginx-storage-demo): pip(andy-k8s197-a3aba87d54c8811e8a78c000d3a01687) - deleting
I0430 15:17:46.295408       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:17:53.340554       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:17:53.340637       1 azure_loadbalancer.go:1192] ensure(default/nginx-storage-demo): pip(andy-k8s197-a3aba87d54c8811e8a78c000d3a01687) - finished
I0430 15:17:53.340665       1 azure_loadbalancer.go:165] delete(default/nginx-storage-demo): FINISH
I0430 15:17:53.340724       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0430 15:17:53.341035       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-storage-demo", UID:"3aba87d5-4c88-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"387432", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0430 15:17:53.341117       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:17:53.341265       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"45ba9554-4c89-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388109", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0430 15:17:53.375214       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:17:53.375237       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0430 15:17:53.375257       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0430 15:17:53.401851       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:17:56.737904       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:17:56.762461       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0430 15:17:56.792977       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:17:56.793011       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0430 15:17:56.793027       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0430 15:17:57.025654       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0430 15:17:57.025704       1 azure_backoff.go:58] backoff: success
I0430 15:17:57.193052       1 azure_backoff.go:58] backoff: success
I0430 15:20:07.691773       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:20:18.085564       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:20:18.085855       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"45ba9554-4c89-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388109", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0430 15:22:14.290451       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0430 15:22:14.291895       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"45ba9554-4c89-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388109", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0430 15:22:14.471209       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:22:15.073768       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-784cc9d865, need 0, deleting 1
I0430 15:22:15.073807       1 controller_utils.go:590] Controller nginx-server-storage-784cc9d865 deleting pod default/nginx-server-storage-784cc9d865-7fpm6
I0430 15:22:15.074174       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"45b4257c-4c89-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388706", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-784cc9d865 to 0
I0430 15:22:15.076556       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0430 15:22:15.086542       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-784cc9d865", UID:"45b54cd7-4c89-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388708", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-784cc9d865-7fpm6
I0430 15:22:18.091038       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-784cc9d865-7fpm6, uid: 45b80e15-4c89-11e8-a78c-000d3a016870]
I0430 15:22:18.098983       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0430 15:22:24.837876       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:22:24.837971       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:22:24.890883       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:22:24.915280       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0430 15:22:25.123432       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:22:25.132075       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:22:25.297289       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0430 15:22:54.016448       1 attacher.go:149] azureDisk - VolumesAreAttached: check volume "andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (specName: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached
I0430 15:22:54.016493       1 operation_generator.go:194] VerifyVolumesAreAttached determined volume "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd" (spec.Name: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached to node "k8s-agentpool-33591117-0", therefore it was marked as detached.
I0430 15:23:08.368780       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-68f66455c, need 1, creating 1
I0430 15:23:08.371969       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"62eb0004-4c8a-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388785", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-68f66455c to 1
I0430 15:23:08.391980       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0430 15:23:08.399886       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-68f66455c", UID:"62ec5a08-4c8a-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"388786", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-68f66455c-hcz6l
I0430 15:23:08.444333       1 endpoints_controller.go:375] Error syncing endpoints for service "default/nginx-server-storage": endpoints "nginx-server-storage" already exists
I0430 15:25:05.834592       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:25:05.834628       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:25:05.942408       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:25:05.942439       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a45ba95544c8911e8a78c000d3a01687) - deleting
I0430 15:25:16.244634       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:25:16.244730       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a45ba95544c8911e8a78c000d3a01687) - finished
I0430 15:25:16.244758       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0430 15:25:16.244892       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0430 15:25:16.244966       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:25:16.245154       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"45ba9554-4c89-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388109", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0430 15:25:16.245227       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"62f16a34-4c8a-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388794", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0430 15:25:16.326932       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:25:16.326982       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0430 15:25:16.327011       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0430 15:25:16.356183       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:25:19.286883       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0430 15:25:19.286918       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:25:19.360001       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:25:19.413910       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0430 15:25:19.995035       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:25:20.040082       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0430 15:25:20.088810       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:25:20.088852       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0430 15:25:20.088891       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0430 15:25:21.031618       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0430 15:25:21.031663       1 azure_backoff.go:58] backoff: success
I0430 15:25:21.128486       1 azure_backoff.go:58] backoff: success
I0430 15:25:38.692597       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:27:41.839844       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:27:52.222576       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:27:52.222995       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"62f16a34-4c8a-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388794", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0430 15:33:33.218953       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-68f66455c, need 0, deleting 1
I0430 15:33:33.219011       1 controller_utils.go:590] Controller nginx-server-storage-68f66455c deleting pod default/nginx-server-storage-68f66455c-hcz6l
I0430 15:33:33.219545       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"62eb0004-4c8a-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"389643", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-68f66455c to 0
I0430 15:33:33.242090       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-68f66455c", UID:"62ec5a08-4c8a-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"389644", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-68f66455c-hcz6l
I0430 15:33:36.248478       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-68f66455c-hcz6l, uid: 62efd195-4c8a-11e8-a78c-000d3a016870]
I0430 15:33:36.264012       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0430 15:33:36.642264       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:33:36.649677       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:33:36.772524       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0430 15:33:41.005717       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0430 15:33:41.005999       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"62f16a34-4c8a-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388794", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0430 15:33:41.247797       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:33:51.619173       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:33:51.619204       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:33:51.650667       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:33:51.674417       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0430 15:33:54.392152       1 attacher.go:149] azureDisk - VolumesAreAttached: check volume "andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (specName: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached
I0430 15:33:54.392195       1 operation_generator.go:194] VerifyVolumesAreAttached determined volume "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd" (spec.Name: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached to node "k8s-agentpool-33591117-0", therefore it was marked as detached.
I0430 15:34:03.442819       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0430 15:34:03.442863       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:35:41.439908       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-9dd9f6db9, need 1, creating 1
I0430 15:35:41.440011       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"23c60a6b-4c8c-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"389828", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-9dd9f6db9 to 1
I0430 15:35:41.463635       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0430 15:35:41.474748       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-9dd9f6db9", UID:"23c75d56-4c8c-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"389830", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-9dd9f6db9-qtcjw
I0430 15:35:41.592910       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:35:41.745238       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0430 15:36:22.288131       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0430 15:36:22.535002       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:36:22.535069       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:36:22.670935       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:36:22.670956       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a62f16a344c8a11e8a78c000d3a01687) - deleting
I0430 15:36:32.992372       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:36:32.992388       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a62f16a344c8a11e8a78c000d3a01687) - finished
I0430 15:36:32.992396       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0430 15:36:32.992430       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0430 15:36:32.992457       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:36:32.992796       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"62f16a34-4c8a-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"388794", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0430 15:36:32.992813       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"23ca2841-4c8c-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"389832", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0430 15:36:33.060745       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:36:33.060817       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0430 15:36:33.060848       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0430 15:36:33.088193       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:36:36.818505       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:36:36.844519       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0430 15:36:36.873687       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:36:36.873717       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0430 15:36:36.873736       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0430 15:36:37.609330       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0430 15:36:37.645499       1 azure_backoff.go:58] backoff: success
I0430 15:36:37.831228       1 azure_backoff.go:58] backoff: success
I0430 15:36:53.994833       1 replica_set.go:349] ReplicaSet "nginx-server-storage-9dd9f6db9" will be enqueued after 5s for availability check
I0430 15:38:28.315858       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:38:38.683387       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:38:38.683828       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"23ca2841-4c8c-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"389832", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
W0430 15:46:16.293957       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0430 15:50:05.780921       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0430 15:50:05.781492       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"23ca2841-4c8c-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"389832", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0430 15:50:06.037976       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:50:06.758334       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-9dd9f6db9, need 0, deleting 1
I0430 15:50:06.758371       1 controller_utils.go:590] Controller nginx-server-storage-9dd9f6db9 deleting pod default/nginx-server-storage-9dd9f6db9-qtcjw
I0430 15:50:06.760161       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"23c60a6b-4c8c-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"390962", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-9dd9f6db9 to 0
I0430 15:50:06.769314       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0430 15:50:06.776289       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-9dd9f6db9", UID:"23c75d56-4c8c-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"390964", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-9dd9f6db9-qtcjw
I0430 15:50:09.267276       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:50:09.270855       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:50:09.277027       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0430 15:50:09.780047       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-9dd9f6db9-qtcjw, uid: 23cdc8db-4c8c-11e8-a78c-000d3a016870]
I0430 15:50:09.785231       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0430 15:50:16.540710       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:50:16.540740       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0430 15:50:16.575445       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0430 15:50:16.604585       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0430 15:50:35.679303       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0430 15:50:35.679345       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0430 15:52:47.659358       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:52:47.659390       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0430 15:52:47.757048       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0430 15:52:47.757070       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a23ca28414c8c11e8a78c000d3a01687) - deleting
I0430 15:52:58.045248       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0430 15:52:58.045276       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a23ca28414c8c11e8a78c000d3a01687) - finished
I0430 15:52:58.045284       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0430 15:52:58.045324       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"23ca2841-4c8c-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"389832", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
W0430 16:02:15.352396       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 16:46:10.401917       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 17:08:10.496959       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 17:22:59.596045       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 17:34:22.609405       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 17:42:39.630151       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 17:57:35.711149       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 18:14:24.818945       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 18:20:57.863222       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 18:34:08.875561       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 18:42:26.924158       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 18:54:50.935529       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 19:00:21.989459       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 19:13:40.019091       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 19:27:40.078388       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 19:42:54.114709       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 19:58:00.124283       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 20:07:55.151627       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 20:15:28.248578       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 20:30:52.325476       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 20:47:26.417459       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 21:02:13.531046       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 21:18:52.593183       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 21:26:49.671996       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 21:41:12.731312       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 21:55:27.813412       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 22:11:15.883539       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 22:22:53.980553       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 22:32:08.084412       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 22:45:07.185503       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 22:56:56.284832       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 23:12:11.324448       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 23:26:57.400715       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 23:42:03.506089       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0430 23:50:17.542231       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 00:07:07.569501       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 00:23:21.604840       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 00:32:24.670314       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 00:46:37.688429       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 01:00:19.795894       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 01:17:19.863069       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 01:25:09.926061       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 01:38:02.009451       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 01:50:38.093217       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 02:05:21.117063       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 02:17:15.140367       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 02:30:28.175827       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 02:42:11.238069       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 02:54:55.294746       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:04:01.336620       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:11:06.400880       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:25:36.506328       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:39:18.581152       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:49:17.662116       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 03:56:31.716629       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 04:13:14.743089       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 04:20:32.837568       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 04:30:32.869686       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 04:43:51.970468       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 04:56:44.019782       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 05:08:15.123564       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 05:17:21.215797       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 05:31:22.269359       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 05:42:57.364403       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 05:51:04.436968       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:04:34.487626       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:14:16.510518       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:24:01.536786       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:31:19.608642       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:47:31.630921       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 06:55:44.696914       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:09:55.726521       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:16:56.793985       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:26:44.833881       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:39:40.903175       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:46:59.973608       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 07:56:08.008787       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 08:08:21.094176       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 08:15:39.128635       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 08:31:39.221248       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 08:46:06.317751       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 09:00:55.394247       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 09:15:15.477708       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 09:28:03.584071       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 09:43:44.599264       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 09:52:49.674095       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:00:45.752745       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:13:47.766832       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:22:19.836274       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:38:47.895279       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:48:34.989238       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 10:56:31.007417       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 11:13:22.031890       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 11:20:19.080284       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 11:38:31.162335       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 11:46:49.192888       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 12:00:43.260736       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 12:10:11.293415       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 12:22:15.345570       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 12:33:24.404557       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 12:41:44.434379       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0501 12:45:14.007376       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"7e2c55e0-4d3d-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"486435", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-69cb7d888c to 1
I0501 12:45:14.016445       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-69cb7d888c, need 1, creating 1
I0501 12:45:14.031757       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 12:45:14.032411       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0501 12:45:14.032461       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 12:45:14.033150       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"7e303dff-4d3d-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"486441", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0501 12:45:14.071412       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-69cb7d888c", UID:"7e2d10fd-4d3d-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"486436", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-69cb7d888c-b446z
I0501 12:45:14.104283       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 12:45:14.639489       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:45:14.639530       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0501 12:45:14.639549       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0501 12:45:14.675374       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0501 12:45:14.700524       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 12:45:18.258438       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:45:18.286625       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0501 12:45:18.322767       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:45:18.322797       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0501 12:45:18.322876       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0501 12:45:18.957972       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0501 12:45:18.958199       1 azure_backoff.go:58] backoff: success
I0501 12:45:19.345433       1 azure_backoff.go:58] backoff: success
I0501 12:45:34.223724       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 12:47:55.694118       1 replica_set.go:349] ReplicaSet "nginx-server-storage-69cb7d888c" will be enqueued after 5s for availability check
I0501 12:48:10.072377       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 12:48:20.429484       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:48:20.429796       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"7e303dff-4d3d-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"486441", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0501 12:50:53.069228       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0501 12:50:53.069959       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"7e303dff-4d3d-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"486441", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0501 12:50:53.138235       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:50:53.996279       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"7e2c55e0-4d3d-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"486907", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-69cb7d888c to 0
I0501 12:50:53.996527       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-69cb7d888c, need 0, deleting 1
I0501 12:50:53.996553       1 controller_utils.go:590] Controller nginx-server-storage-69cb7d888c deleting pod default/nginx-server-storage-69cb7d888c-b446z
I0501 12:50:54.007823       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 12:50:54.015679       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-69cb7d888c", UID:"7e2d10fd-4d3d-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"486909", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-69cb7d888c-b446z
I0501 12:50:57.018895       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-69cb7d888c-b446z, uid: 7e3139b6-4d3d-11e8-a78c-000d3a016870]
I0501 12:50:57.024788       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0501 12:50:57.707362       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 12:50:57.718686       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 12:50:57.768835       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0501 12:51:03.367997       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:51:03.368034       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 12:51:03.396897       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:51:03.417694       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0501 12:51:22.969880       1 attacher.go:149] azureDisk - VolumesAreAttached: check volume "andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (specName: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached
I0501 12:51:22.969925       1 operation_generator.go:194] VerifyVolumesAreAttached determined volume "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd" (spec.Name: "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870") is no longer attached to node "k8s-agentpool-33591117-0", therefore it was marked as detached.
I0501 12:51:24.302067       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0501 12:51:24.302104       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-3ab4281c-4c88-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 12:53:44.243952       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:53:44.244020       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 12:53:44.324629       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 12:53:44.324644       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a7e303dff4d3d11e8a78c000d3a01687) - deleting
I0501 12:53:54.618831       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:53:54.618852       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a7e303dff4d3d11e8a78c000d3a01687) - finished
I0501 12:53:54.618860       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0501 12:53:54.619017       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"7e303dff-4d3d-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"486441", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0501 12:55:17.514615       1 pv_controller.go:542] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 12:55:17.522279       1 pv_controller.go:754] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" entered phase "Released"
I0501 12:55:17.529198       1 pv_controller.go:1220] isVolumeReleased[pvc-3ab4281c-4c88-11e8-a78c-000d3a016870]: volume is released
I0501 12:55:17.595347       1 pv_controller.go:1259] volume "pvc-3ab4281c-4c88-11e8-a78c-000d3a016870" deleted
I0501 12:55:17.599860       1 pv_controller_base.go:376] deletion of claim "default/pvc-azuredisk" was already processed
I0501 12:55:21.342844       1 pv_controller.go:542] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 12:55:21.346239       1 pv_controller.go:754] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" entered phase "Released"
I0501 12:55:21.348240       1 pv_controller.go:1220] isVolumeReleased[pvc-3ab5e198-4c88-11e8-a78c-000d3a016870]: volume is released
I0501 12:55:21.403917       1 pv_controller.go:1259] volume "pvc-3ab5e198-4c88-11e8-a78c-000d3a016870" deleted
I0501 12:55:21.412177       1 pv_controller_base.go:376] deletion of claim "default/pvc-azurefile" was already processed
I0501 12:55:46.508682       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-69cb7d888c, need 1, creating 1
I0501 12:55:46.512707       1 azure_blobDiskController.go:537] azureDisk - account ds4c1e71e849cb11e8b4640 identified for a new disk  is because it has 0 allocated disks
I0501 12:55:46.520100       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"f72ba28c-4d3e-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"487306", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-69cb7d888c to 1
I0501 12:55:46.538801       1 pv_controller.go:1462] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azuredisk"
I0501 12:55:46.539571       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azuredisk", UID:"f7286f6c-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487305", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870 using kubernetes.io/azure-disk
I0501 12:55:46.549837       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-69cb7d888c", UID:"f72d7653-4d3e-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"487307", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-69cb7d888c-dmcmn
I0501 12:55:46.557779       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0501 12:55:46.558119       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 12:55:46.558852       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"f7332b99-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487315", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0501 12:55:46.560458       1 pv_controller.go:754] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 12:55:46.560506       1 pv_controller.go:893] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" bound to claim "default/pvc-azuredisk"
I0501 12:55:46.576665       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 12:55:46.600469       1 pv_controller.go:698] claim "default/pvc-azuredisk" entered phase "Bound"
I0501 12:55:46.658226       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:55:46.658303       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0501 12:55:46.658388       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0501 12:55:46.691852       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 12:55:46.769488       1 pv_controller.go:1462] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azurefile"
I0501 12:55:46.769813       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azurefile", UID:"f729ce58-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487304", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-f729ce58-4d3e-11e8-a78c-000d3a016870 using kubernetes.io/azure-file
I0501 12:55:46.778784       1 pv_controller.go:754] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 12:55:46.778812       1 pv_controller.go:893] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" bound to claim "default/pvc-azurefile"
I0501 12:55:46.786420       1 pv_controller.go:698] claim "default/pvc-azurefile" entered phase "Bound"
I0501 12:55:47.606378       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 12:55:47.693574       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0501 12:55:50.857101       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:55:50.952322       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0501 12:55:51.011231       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 12:55:51.011257       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0501 12:55:51.011270       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0501 12:55:51.318309       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0501 12:55:51.318346       1 azure_backoff.go:58] backoff: success
I0501 12:55:51.982076       1 azure_backoff.go:58] backoff: success
I0501 12:56:07.379672       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 12:58:28.919400       1 replica_set.go:349] ReplicaSet "nginx-server-storage-69cb7d888c" will be enqueued after 5s for availability check
I0501 12:58:42.256789       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 12:58:52.535548       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 12:58:52.535887       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"f7332b99-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487315", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0501 13:01:53.823185       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0501 13:01:53.824514       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"f7332b99-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487315", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0501 13:01:53.925713       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:01:55.048016       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-69cb7d888c, need 0, deleting 1
I0501 13:01:55.048052       1 controller_utils.go:590] Controller nginx-server-storage-69cb7d888c deleting pod default/nginx-server-storage-69cb7d888c-dmcmn
I0501 13:01:55.049263       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"f72ba28c-4d3e-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"487824", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-69cb7d888c to 0
I0501 13:01:55.061073       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 13:01:55.071480       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-69cb7d888c", UID:"f72d7653-4d3e-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"487826", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-69cb7d888c-dmcmn
I0501 13:01:58.074528       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-69cb7d888c-dmcmn, uid: f72f9e51-4d3e-11e8-a78c-000d3a016870]
I0501 13:01:58.079619       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0501 13:01:59.586476       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:01:59.594106       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:01:59.671678       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0501 13:02:04.200073       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:02:04.200095       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 13:02:04.244140       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:02:04.290496       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0501 13:02:05.482823       1 pv_controller.go:542] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 13:02:05.492640       1 pv_controller.go:754] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" entered phase "Released"
I0501 13:02:05.501005       1 pv_controller.go:1220] isVolumeReleased[pvc-f729ce58-4d3e-11e8-a78c-000d3a016870]: volume is released
I0501 13:02:05.568384       1 pv_controller.go:1259] volume "pvc-f729ce58-4d3e-11e8-a78c-000d3a016870" deleted
I0501 13:02:05.571696       1 pv_controller_base.go:376] deletion of claim "default/pvc-azurefile" was already processed
I0501 13:02:15.932493       1 pv_controller.go:542] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 13:02:15.938136       1 pv_controller.go:754] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" entered phase "Released"
I0501 13:02:15.946258       1 pv_controller.go:1220] isVolumeReleased[pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870]: volume is released
I0501 13:02:16.016205       1 pv_controller.go:754] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" entered phase "Failed"
E0501 13:02:16.016315       1 goroutinemap.go:165] Operation for "delete-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870[f7310cc1-4d3e-11e8-a78c-000d3a016870]" failed. No retries permitted until 2018-05-01 13:02:16.516285649 +0000 UTC m=+384221.113773394 (durationBeforeRetry 500ms). Error: "storage: service returned error: StatusCode=412, ErrorCode=LeaseIdMissing, ErrorMessage=There is currently a lease on the blob and no lease ID was specified in the request.\nRequestId:f063dc8f-001e-004b-5f4c-e1045e000000\nTime:2018-05-01T13:02:16.0139614Z, RequestInitiated=Tue, 01 May 2018 13:02:15 GMT, RequestId=f063dc8f-001e-004b-5f4c-e1045e000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue="
I0501 13:02:16.016569       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolume", Namespace:"", Name:"pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870", UID:"f7310cc1-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487876", FieldPath:""}): type: 'Warning' reason: 'VolumeFailedDelete' storage: service returned error: StatusCode=412, ErrorCode=LeaseIdMissing, ErrorMessage=There is currently a lease on the blob and no lease ID was specified in the request.
RequestId:f063dc8f-001e-004b-5f4c-e1045e000000
Time:2018-05-01T13:02:16.0139614Z, RequestInitiated=Tue, 01 May 2018 13:02:15 GMT, RequestId=f063dc8f-001e-004b-5f4c-e1045e000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue=
I0501 13:02:20.815828       1 pv_controller.go:1220] isVolumeReleased[pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870]: volume is released
I0501 13:02:20.822253       1 pv_controller.go:1259] volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" deleted
I0501 13:02:20.826000       1 pv_controller_base.go:376] deletion of claim "default/pvc-azuredisk" was already processed
I0501 13:02:23.286983       1 attacher.go:149] azureDisk - VolumesAreAttached: check volume "andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (specName: "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870") is no longer attached
I0501 13:02:23.287023       1 operation_generator.go:194] VerifyVolumesAreAttached determined volume "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd" (spec.Name: "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870") is no longer attached to node "k8s-agentpool-33591117-0", therefore it was marked as detached.
I0501 13:02:25.879340       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0501 13:02:25.879399       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-f7286f6c-4d3e-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:04:45.156747       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:04:45.156770       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 13:04:45.232347       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 13:04:45.232435       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-af7332b994d3e11e8a78c000d3a01687) - deleting
I0501 13:04:55.459183       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:04:55.459282       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-af7332b994d3e11e8a78c000d3a01687) - finished
I0501 13:04:55.459325       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0501 13:04:55.459458       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"f7332b99-4d3e-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"487315", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0501 13:07:09.447154       1 azure_blobDiskController.go:537] azureDisk - account ds4c1e71e849cb11e8b4640 identified for a new disk  is because it has 0 allocated disks
I0501 13:07:09.454017       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"8e3bf76f-4d40-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"488251", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-5c5c879755 to 1
I0501 13:07:09.454250       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-5c5c879755, need 1, creating 1
I0501 13:07:09.467631       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0501 13:07:09.467667       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 13:07:09.468157       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"8e3f9615-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488257", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0501 13:07:09.477903       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 13:07:09.502852       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-5c5c879755", UID:"8e3d92d9-4d40-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"488253", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-5c5c879755-l5wkc
I0501 13:07:09.518585       1 pv_controller.go:1462] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azuredisk"
I0501 13:07:09.518872       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azuredisk", UID:"8e390821-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488248", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-8e390821-4d40-11e8-a78c-000d3a016870 using kubernetes.io/azure-disk
I0501 13:07:09.527818       1 pv_controller.go:754] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 13:07:09.527863       1 pv_controller.go:893] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" bound to claim "default/pvc-azuredisk"
I0501 13:07:09.536900       1 pv_controller.go:698] claim "default/pvc-azuredisk" entered phase "Bound"
I0501 13:07:09.588904       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:07:09.589013       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0501 13:07:09.589075       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0501 13:07:09.616316       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 13:07:09.798642       1 pv_controller.go:1462] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azurefile"
I0501 13:07:09.798939       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azurefile", UID:"8e3a5e43-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488250", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870 using kubernetes.io/azure-file
I0501 13:07:09.802465       1 pv_controller.go:754] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 13:07:09.802607       1 pv_controller.go:893] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" bound to claim "default/pvc-azurefile"
I0501 13:07:09.814730       1 pv_controller.go:698] claim "default/pvc-azurefile" entered phase "Bound"
I0501 13:07:10.535566       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 13:07:10.636364       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0501 13:07:13.067524       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:07:13.108609       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0501 13:07:13.138448       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:07:13.138522       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0501 13:07:13.138791       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0501 13:07:13.407373       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0501 13:07:13.407541       1 azure_backoff.go:58] backoff: success
I0501 13:07:13.502664       1 azure_backoff.go:58] backoff: success
I0501 13:07:37.004952       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 13:10:04.261849       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 13:10:14.599625       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:10:14.599970       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"8e3f9615-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488257", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0501 13:10:34.309919       1 replica_set.go:349] ReplicaSet "nginx-server-storage-5c5c879755" will be enqueued after 5s for availability check
W0501 13:26:43.534142       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 13:35:37.771542       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0501 13:50:16.180376       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0501 13:50:16.198038       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"8e3f9615-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488257", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0501 13:50:16.662893       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"8e3bf76f-4d40-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"491529", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-5c5c879755 to 0
I0501 13:50:16.662919       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:50:16.676790       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 13:50:16.676743       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-5c5c879755, need 0, deleting 1
I0501 13:50:16.677251       1 controller_utils.go:590] Controller nginx-server-storage-5c5c879755 deleting pod default/nginx-server-storage-5c5c879755-l5wkc
I0501 13:50:16.713441       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-5c5c879755", UID:"8e3d92d9-4d40-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"491531", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-5c5c879755-l5wkc
I0501 13:50:19.563323       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-5c5c879755-l5wkc, uid: 8e40b70d-4d40-11e8-a78c-000d3a016870]
I0501 13:50:19.567449       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0501 13:50:23.247776       1 pv_controller.go:542] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 13:50:23.276585       1 pv_controller.go:754] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" entered phase "Released"
I0501 13:50:23.278438       1 pv_controller.go:1220] isVolumeReleased[pvc-8e390821-4d40-11e8-a78c-000d3a016870]: volume is released
I0501 13:50:23.399071       1 pv_controller.go:754] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" entered phase "Failed"
E0501 13:50:23.399123       1 goroutinemap.go:165] Operation for "delete-pvc-8e390821-4d40-11e8-a78c-000d3a016870[8e472edf-4d40-11e8-a78c-000d3a016870]" failed. No retries permitted until 2018-05-01 13:50:23.89910031 +0000 UTC m=+387108.496588055 (durationBeforeRetry 500ms). Error: "storage: service returned error: StatusCode=412, ErrorCode=LeaseIdMissing, ErrorMessage=There is currently a lease on the blob and no lease ID was specified in the request.\nRequestId:273fe7d3-401e-00cc-1553-e1973b000000\nTime:2018-05-01T13:50:23.3388300Z, RequestInitiated=Tue, 01 May 2018 13:50:23 GMT, RequestId=273fe7d3-401e-00cc-1553-e1973b000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue="
I0501 13:50:23.399209       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolume", Namespace:"", Name:"pvc-8e390821-4d40-11e8-a78c-000d3a016870", UID:"8e472edf-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"491552", FieldPath:""}): type: 'Warning' reason: 'VolumeFailedDelete' storage: service returned error: StatusCode=412, ErrorCode=LeaseIdMissing, ErrorMessage=There is currently a lease on the blob and no lease ID was specified in the request.
RequestId:273fe7d3-401e-00cc-1553-e1973b000000
Time:2018-05-01T13:50:23.3388300Z, RequestInitiated=Tue, 01 May 2018 13:50:23 GMT, RequestId=273fe7d3-401e-00cc-1553-e1973b000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue=
I0501 13:50:25.032881       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:50:25.041271       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:50:25.101671       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0501 13:50:27.048071       1 pv_controller.go:542] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" is released and reclaim policy "Delete" will be executed
I0501 13:50:27.051391       1 pv_controller.go:754] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" entered phase "Released"
I0501 13:50:27.053447       1 pv_controller.go:1220] isVolumeReleased[pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870]: volume is released
I0501 13:50:27.132142       1 pv_controller.go:1259] volume "pvc-8e3a5e43-4d40-11e8-a78c-000d3a016870" deleted
I0501 13:50:27.135826       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:50:27.135958       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 13:50:27.136315       1 pv_controller_base.go:376] deletion of claim "default/pvc-azurefile" was already processed
I0501 13:50:27.166436       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:50:27.189263       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0501 13:50:35.861233       1 pv_controller.go:1220] isVolumeReleased[pvc-8e390821-4d40-11e8-a78c-000d3a016870]: volume is released
E0501 13:50:35.864830       1 goroutinemap.go:165] Operation for "delete-pvc-8e390821-4d40-11e8-a78c-000d3a016870[8e472edf-4d40-11e8-a78c-000d3a016870]" failed. No retries permitted until 2018-05-01 13:50:36.864799551 +0000 UTC m=+387121.462287296 (durationBeforeRetry 1s). Error: "storage: service returned error: StatusCode=412, ErrorCode=LeaseIdMissing, ErrorMessage=There is currently a lease on the blob and no lease ID was specified in the request.\nRequestId:273ff47b-401e-00cc-4253-e1973b000000\nTime:2018-05-01T13:50:35.8656688Z, RequestInitiated=Tue, 01 May 2018 13:50:35 GMT, RequestId=273ff47b-401e-00cc-4253-e1973b000000, API Version=2016-05-31, QueryParameterName=, QueryParameterValue="
I0501 13:50:50.861375       1 pv_controller.go:1220] isVolumeReleased[pvc-8e390821-4d40-11e8-a78c-000d3a016870]: volume is released
W0501 13:50:50.870471       1 azure_blobDiskController.go:268] azureDisk - failed to get disk count for ds4c1e71e849cb11e8b4640 however the delete disk operation was ok
I0501 13:50:50.870487       1 pv_controller.go:1259] volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" deleted
I0501 13:50:50.873555       1 pv_controller_base.go:376] deletion of claim "default/pvc-azuredisk" was already processed
I0501 13:50:51.600616       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0501 13:50:51.600786       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-8e390821-4d40-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-8e390821-4d40-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 13:53:08.105076       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:53:08.105100       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 13:53:08.191123       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 13:53:08.191179       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a8e3f96154d4011e8a78c000d3a01687) - deleting
I0501 13:53:18.396127       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:53:18.396144       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a8e3f96154d4011e8a78c000d3a01687) - finished
I0501 13:53:18.396152       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0501 13:53:18.396209       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"8e3f9615-4d40-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"488257", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0501 13:55:22.931014       1 event.go:218] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"flex", Name:"blobfuse-flexvol-installer", UID:"4ad0113d-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"491951", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: blobfuse-flexvol-installer-6n5rb
I0501 13:55:22.937410       1 event.go:218] Event(v1.ObjectReference{Kind:"DaemonSet", Namespace:"flex", Name:"blobfuse-flexvol-installer", UID:"4ad0113d-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"491951", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: blobfuse-flexvol-installer-wvlwb
I0501 13:56:56.188275       1 azure_blobDiskController.go:537] azureDisk - account ds4c1e71e849cb11e8b4640 identified for a new disk  is because it has 0 allocated disks
I0501 13:56:56.199626       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-storage-f6b4c7bd4, need 1, creating 1
I0501 13:56:56.200682       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"8279c9dd-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"492116", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-storage-f6b4c7bd4 to 1
I0501 13:56:56.225740       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 13:56:56.229760       1 service_controller.go:301] Ensuring LB for service default/nginx-server-storage
I0501 13:56:56.232766       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"82802b5b-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492123", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0501 13:56:56.239631       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-f6b4c7bd4", UID:"827bda42-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"492117", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-storage-f6b4c7bd4-c2kgd
I0501 13:56:56.279135       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 13:56:56.461919       1 pv_controller.go:1462] volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azuredisk"
I0501 13:56:56.462057       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azuredisk", UID:"82770939-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492114", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-82770939-4d47-11e8-a78c-000d3a016870 using kubernetes.io/azure-disk
I0501 13:56:56.470879       1 pv_controller.go:754] volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 13:56:56.470901       1 pv_controller.go:893] volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" bound to claim "default/pvc-azuredisk"
I0501 13:56:56.474336       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:56:56.474353       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0501 13:56:56.474372       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197-internal) finished
I0501 13:56:56.476110       1 pv_controller.go:698] claim "default/pvc-azuredisk" entered phase "Bound"
I0501 13:56:56.550265       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 13:56:56.698915       1 pv_controller.go:1462] volume "pvc-82781df4-4d47-11e8-a78c-000d3a016870" provisioned for claim "default/pvc-azurefile"
I0501 13:56:56.698983       1 event.go:218] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"pvc-azurefile", UID:"82781df4-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492115", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-82781df4-4d47-11e8-a78c-000d3a016870 using kubernetes.io/azure-file
I0501 13:56:56.707618       1 pv_controller.go:754] volume "pvc-82781df4-4d47-11e8-a78c-000d3a016870" entered phase "Bound"
I0501 13:56:56.707659       1 pv_controller.go:893] volume "pvc-82781df4-4d47-11e8-a78c-000d3a016870" bound to claim "default/pvc-azurefile"
I0501 13:56:56.721490       1 pv_controller.go:698] claim "default/pvc-azurefile" entered phase "Bound"
I0501 13:56:57.326991       1 reconciler.go:287] attacherDetacher.AttachVolume started for volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 13:56:57.483659       1 azure_controllerCommon.go:114] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - attach disk
I0501 13:56:59.932428       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:56:59.954699       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(true): started
I0501 13:56:59.980129       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 13:56:59.980170       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server-storage) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0501 13:56:59.980235       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0501 13:57:00.463513       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0501 13:57:00.463696       1 azure_backoff.go:58] backoff: success
I0501 13:57:00.618182       1 azure_backoff.go:58] backoff: success
I0501 13:57:24.027367       1 operation_generator.go:309] AttachVolume.Attach succeeded for volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd") from node "k8s-agentpool-33591117-0" 
I0501 13:59:41.839462       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 13:59:52.192770       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 13:59:52.193194       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"82802b5b-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492123", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
I0501 14:00:49.330715       1 service_controller.go:763] Service has been deleted default/nginx-server-storage
I0501 14:00:49.333600       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"82802b5b-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492123", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0501 14:00:49.414093       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 14:00:50.240867       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server-storage", UID:"8279c9dd-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"492467", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-storage-f6b4c7bd4 to 0
I0501 14:00:50.242108       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-storage-f6b4c7bd4, need 0, deleting 1
I0501 14:00:50.242146       1 controller_utils.go:590] Controller nginx-server-storage-f6b4c7bd4 deleting pod default/nginx-server-storage-f6b4c7bd4-c2kgd
I0501 14:00:50.250619       1 deployment_controller.go:485] Error syncing deployment default/nginx-server-storage: Operation cannot be fulfilled on deployments.extensions "nginx-server-storage": the object has been modified; please apply your changes to the latest version and try again
I0501 14:00:50.260209       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-storage-f6b4c7bd4", UID:"827bda42-4d47-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"492469", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-storage-f6b4c7bd4-c2kgd
I0501 14:00:53.261573       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-storage-f6b4c7bd4-c2kgd, uid: 827fff00-4d47-11e8-a78c-000d3a016870]
I0501 14:00:53.273065       1 deployment_controller.go:574] Deployment default/nginx-server-storage has been deleted
I0501 14:00:56.613692       1 reconciler.go:231] attacherDetacher.DetachVolume started for volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 14:00:56.622100       1 operation_generator.go:1165] Verified volume is safe to detach for volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 14:00:56.679636       1 azure_controllerCommon.go:180] azureDisk - update(andy-k8s197): vm(k8s-agentpool-33591117-0) - detach disk
I0501 14:00:59.583009       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 14:00:59.583036       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server-storage) - wantLb(false): started
I0501 14:00:59.615956       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0501 14:00:59.639695       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server-storage): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0501 14:01:25.508185       1 attacher.go:149] azureDisk - VolumesAreAttached: check volume "andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870" (specName: "pvc-82770939-4d47-11e8-a78c-000d3a016870") is no longer attached
I0501 14:01:25.508224       1 operation_generator.go:194] VerifyVolumesAreAttached determined volume "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd" (spec.Name: "pvc-82770939-4d47-11e8-a78c-000d3a016870") is no longer attached to node "k8s-agentpool-33591117-0", therefore it was marked as detached.
I0501 14:01:44.178650       1 attacher.go:298] azureDisk - disk:https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd was detached from node:k8s-agentpool-33591117-0
I0501 14:01:44.178754       1 operation_generator.go:387] DetachVolume.Detach succeeded for volume "pvc-82770939-4d47-11e8-a78c-000d3a016870" (UniqueName: "kubernetes.io/azure-disk/https://ds4c1e71e849cb11e8b4640.blob.core.windows.net/vhds/andy-k8s197-dynamic-pvc-82770939-4d47-11e8-a78c-000d3a016870.vhd") on node "k8s-agentpool-33591117-0" 
I0501 14:04:00.559555       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 14:04:00.559613       1 azure_loadbalancer.go:800] ensure(default/nginx-server-storage): lb(andy-k8s197) finished
I0501 14:04:00.692137       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0501 14:04:00.692208       1 azure_loadbalancer.go:1177] ensure(default/nginx-server-storage): pip(andy-k8s197-a82802b5b4d4711e8a78c000d3a01687) - deleting
I0501 14:04:10.888172       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0501 14:04:10.888192       1 azure_loadbalancer.go:1192] ensure(default/nginx-server-storage): pip(andy-k8s197-a82802b5b4d4711e8a78c000d3a01687) - finished
I0501 14:04:10.888200       1 azure_loadbalancer.go:165] delete(default/nginx-server-storage): FINISH
I0501 14:04:10.888256       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server-storage", UID:"82802b5b-4d47-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"492123", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
W0501 14:22:06.868101       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 14:30:58.945200       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 14:45:19.038216       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 15:15:40.057613       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 15:28:17.157622       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 15:42:38.226791       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 15:51:09.304788       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 16:00:55.379322       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 16:14:24.448584       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 16:23:54.473126       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 16:30:33.481161       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 16:46:53.568009       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:00:50.623912       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:15:05.703505       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:29:16.776288       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:36:21.860263       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:48:16.946238       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 17:55:19.007897       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 18:10:21.043637       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 18:27:22.124054       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 18:36:20.220611       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 18:45:49.247260       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 18:58:59.309452       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 19:06:47.367520       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 19:15:19.433611       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 19:32:05.454425       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 19:40:56.540398       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 19:55:11.620001       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 20:09:32.683800       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 20:16:20.717081       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 20:28:38.770501       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 20:38:33.854564       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 20:51:41.903532       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 21:01:33.008001       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 21:11:13.070246       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 21:28:18.151353       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 21:37:42.258333       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 21:48:43.297250       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 22:03:10.312008       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 22:16:57.394161       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 22:34:12.424589       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 22:41:03.476992       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 22:56:08.547411       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 23:10:03.595881       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 23:25:15.699442       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 23:39:52.728925       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 23:46:57.798373       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0501 23:55:33.825065       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 00:08:38.915412       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 00:15:45.008954       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 00:31:24.082242       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 00:44:15.106632       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 00:52:46.145045       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 01:07:04.219373       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 01:20:11.288230       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 01:31:30.344417       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 01:41:09.427567       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 01:52:52.472538       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
W0502 02:01:33.517907       1 reflector.go:341] k8s.io/kubernetes/vendor/k8s.io/client-go/informers/factory.go:86: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old.
I0502 02:09:20.183675       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-6994758849, need 2, creating 2
I0502 02:09:20.185304       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"d321222a-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"547909", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-6994758849 to 2
I0502 02:09:20.204586       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:09:20.204668       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:09:20.205442       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"d325ffb0-4dad-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"547915", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:09:20.220325       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:09:20.229997       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"d32390f4-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"547911", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-6994758849-94js8
I0502 02:09:20.246870       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"d32390f4-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"547911", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-6994758849-7q6t8
I0502 02:09:20.252119       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:09:20.789290       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:09:20.789316       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:09:20.789334       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:09:20.859681       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:09:24.536534       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:09:24.600412       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(true): started
I0502 02:09:24.634591       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:09:24.634703       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0502 02:09:24.634767       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0502 02:09:24.897943       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0502 02:09:25.060828       1 azure_backoff.go:58] backoff: success
I0502 02:09:25.178533       1 azure_backoff.go:58] backoff: success
I0502 02:10:37.907657       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-6994758849, need 0, deleting 2
I0502 02:10:37.907694       1 controller_utils.go:590] Controller nginx-server-6994758849 deleting pod default/nginx-server-6994758849-7q6t8
I0502 02:10:37.907824       1 controller_utils.go:590] Controller nginx-server-6994758849 deleting pod default/nginx-server-6994758849-94js8
I0502 02:10:37.908496       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"d321222a-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548098", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-6994758849 to 0
I0502 02:10:37.915495       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:10:37.930193       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"d32390f4-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548100", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-6994758849-7q6t8
I0502 02:10:37.934252       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"d32390f4-4dad-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548100", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-6994758849-94js8
I0502 02:10:40.924577       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-6994758849-94js8, uid: d326dc03-4dad-11e8-a78c-000d3a016870]
I0502 02:10:40.924638       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-6994758849-7q6t8, uid: d32bdb90-4dad-11e8-a78c-000d3a016870]
I0502 02:10:40.937143       1 deployment_controller.go:574] Deployment default/nginx-server has been deleted
I0502 02:11:43.408889       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-6994758849, need 2, creating 2
I0502 02:11:43.409578       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"2880ceb2-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548202", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-6994758849 to 2
I0502 02:11:43.429597       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"2882239f-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548204", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-6994758849-92v42
I0502 02:11:43.430514       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:11:43.441732       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"2882239f-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548204", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-6994758849-d5r7x
I0502 02:11:55.974655       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197) finished
I0502 02:12:06.267755       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:12:06.267980       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"d325ffb0-4dad-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"547915", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
E0502 02:12:06.271977       1 service_controller.go:784] failed to process service default/nginx-server. Not retrying: failed to persist updated status to apiserver, even after retries. Giving up: not persisting update to service 'default/nginx-server' that has been changed since we received it: Operation cannot be fulfilled on services "nginx-server": StorageError: invalid object, Code: 4, Key: /registry/services/specs/default/nginx-server, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: d325ffb0-4dad-11e8-a78c-000d3a016870, UID in object meta: 28853f7f-4dae-11e8-a78c-000d3a016870
I0502 02:12:06.272066       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"d325ffb0-4dad-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"547915", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will not retry): failed to persist updated status to apiserver, even after retries. Giving up: not persisting update to service 'default/nginx-server' that has been changed since we received it: Operation cannot be fulfilled on services "nginx-server": StorageError: invalid object, Code: 4, Key: /registry/services/specs/default/nginx-server, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: d325ffb0-4dad-11e8-a78c-000d3a016870, UID in object meta: 28853f7f-4dae-11e8-a78c-000d3a016870
I0502 02:12:06.272152       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"d325ffb0-4dad-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"547915", FieldPath:""}): type: 'Normal' reason: 'DeletingLoadBalancer' Deleting load balancer
I0502 02:12:06.308270       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:12:11.716953       1 replica_set.go:526] Too many replicas for ReplicaSet default/nginx-server-6994758849, need 0, deleting 2
I0502 02:12:11.717007       1 controller_utils.go:590] Controller nginx-server-6994758849 deleting pod default/nginx-server-6994758849-d5r7x
I0502 02:12:11.717131       1 controller_utils.go:590] Controller nginx-server-6994758849 deleting pod default/nginx-server-6994758849-92v42
I0502 02:12:11.717710       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"2880ceb2-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548324", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled down replica set nginx-server-6994758849 to 0
I0502 02:12:11.721430       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:12:11.729632       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"2882239f-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548326", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-6994758849-d5r7x
I0502 02:12:11.737501       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-6994758849", UID:"2882239f-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548326", FieldPath:""}): type: 'Normal' reason: 'SuccessfulDelete' Deleted pod: nginx-server-6994758849-92v42
I0502 02:12:11.737838       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:12:14.732068       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-6994758849-92v42, uid: 2885053c-4dae-11e8-a78c-000d3a016870]
I0502 02:12:14.732114       1 garbagecollector.go:364] processing item [v1/Pod, namespace: default, name: nginx-server-6994758849-d5r7x, uid: 2885f2b9-4dae-11e8-a78c-000d3a016870]
I0502 02:12:14.744152       1 deployment_controller.go:574] Deployment default/nginx-server has been deleted
I0502 02:12:16.473212       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:12:16.473235       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:12:16.504429       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:12:16.530482       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(false) resolved load balancer name
I0502 02:12:53.830617       1 replica_set.go:478] Too few replicas for ReplicaSet default/nginx-server-776bb578c, need 2, creating 2
I0502 02:12:53.831941       1 event.go:218] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"default", Name:"nginx-server", UID:"527a650c-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548391", FieldPath:""}): type: 'Normal' reason: 'ScalingReplicaSet' Scaled up replica set nginx-server-776bb578c to 2
I0502 02:12:53.850260       1 deployment_controller.go:485] Error syncing deployment default/nginx-server: Operation cannot be fulfilled on deployments.extensions "nginx-server": the object has been modified; please apply your changes to the latest version and try again
I0502 02:12:53.865103       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-776bb578c", UID:"527bb94a-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548392", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-776bb578c-ckd6b
I0502 02:12:53.886765       1 event.go:218] Event(v1.ObjectReference{Kind:"ReplicaSet", Namespace:"default", Name:"nginx-server-776bb578c", UID:"527bb94a-4dae-11e8-a78c-000d3a016870", APIVersion:"extensions", ResourceVersion:"548392", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: nginx-server-776bb578c-bxsvg
I0502 02:12:53.910063       1 endpoints_controller.go:375] Error syncing endpoints for service "default/nginx-server": endpoints "nginx-server" already exists
I0502 02:12:58.472272       1 replica_set.go:349] ReplicaSet "nginx-server-776bb578c" will be enqueued after 5s for availability check
I0502 02:12:59.478070       1 replica_set.go:349] ReplicaSet "nginx-server-776bb578c" will be enqueued after 5s for availability check
I0502 02:13:57.052730       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:13:57.052759       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197) finished
I0502 02:13:57.134219       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:13:57.134243       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-ad325ffb04dad11e8a78c000d3a01687) - deleting
I0502 02:14:07.499319       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:14:07.499339       1 azure_loadbalancer.go:1192] ensure(default/nginx-server): pip(andy-k8s197-ad325ffb04dad11e8a78c000d3a01687) - finished
I0502 02:14:07.499348       1 azure_loadbalancer.go:165] delete(default/nginx-server): FINISH
I0502 02:14:07.499381       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:14:07.499426       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:14:07.499568       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"d325ffb0-4dad-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"547915", FieldPath:""}): type: 'Normal' reason: 'DeletedLoadBalancer' Deleted load balancer
I0502 02:14:07.499588       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"28853f7f-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548209", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:14:07.609945       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:14:07.610064       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:14:07.610104       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:14:07.644170       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:14:11.632130       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:14:11.661094       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(true): started
I0502 02:14:11.694749       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:14:11.694779       1 azure_loadbalancer.go:243] selectLoadBalancer: cluster(andy-k8s197) service(default/nginx-server) isInternal(false) - availabilitysetsnames [agentpool-availabilitySet-33591117]
I0502 02:14:11.694811       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197) wantLb(true) resolved load balancer name
I0502 02:14:12.158572       1 azure_backoff.go:398] backoff: success, HTTP response=201
I0502 02:14:12.206667       1 azure_backoff.go:58] backoff: success
I0502 02:14:12.362727       1 azure_backoff.go:58] backoff: success
I0502 02:16:02.800919       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197) finished
I0502 02:16:13.096188       1 azure_backoff.go:398] backoff: success, HTTP response=200
I0502 02:16:13.096462       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"28853f7f-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548209", FieldPath:""}): type: 'Normal' reason: 'EnsuredLoadBalancer' Ensured load balancer
E0502 02:16:13.104702       1 service_controller.go:784] failed to process service default/nginx-server. Not retrying: failed to persist updated status to apiserver, even after retries. Giving up: not persisting update to service 'default/nginx-server' that has been changed since we received it: Operation cannot be fulfilled on services "nginx-server": StorageError: invalid object, Code: 4, Key: /registry/services/specs/default/nginx-server, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 28853f7f-4dae-11e8-a78c-000d3a016870, UID in object meta: 527f0e45-4dae-11e8-a78c-000d3a016870
I0502 02:16:13.104729       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:16:13.104758       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:16:13.104830       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"28853f7f-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548209", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will not retry): failed to persist updated status to apiserver, even after retries. Giving up: not persisting update to service 'default/nginx-server' that has been changed since we received it: Operation cannot be fulfilled on services "nginx-server": StorageError: invalid object, Code: 4, Key: /registry/services/specs/default/nginx-server, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 28853f7f-4dae-11e8-a78c-000d3a016870, UID in object meta: 527f0e45-4dae-11e8-a78c-000d3a016870
I0502 02:16:13.105005       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:16:13.273484       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:16:13.273618       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:16:13.273762       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:16:13.329394       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:16:13.329420       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:13.704765       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:16:13.704828       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:13.704854       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:13.704939       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:18.704977       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:16:18.705062       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:16:18.705608       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:16:18.742033       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:16:18.742058       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:16:18.742077       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:16:18.770551       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:16:18.770578       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:18.847910       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:16:18.848036       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
I0502 02:16:18.848600       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
E0502 02:16:18.848733       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 10s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:28.848985       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:16:28.849034       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:16:28.849177       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:16:28.960059       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:16:28.960083       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:16:28.960102       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:16:29.194516       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:16:29.194543       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:29.397715       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:16:29.397792       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:29.397817       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 20s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:29.398341       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:49.398006       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:16:49.398079       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:16:49.398693       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:16:49.437906       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:16:49.437934       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:16:49.437953       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:16:49.464306       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:16:49.464330       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:49.527008       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:16:49.527057       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:16:49.527084       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 40s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:16:49.527165       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:17:29.527336       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:17:29.527410       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:17:29.527897       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:17:29.607171       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:17:29.607233       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:17:29.607254       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:17:29.659249       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:17:29.659335       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:17:29.850069       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:17:29.850122       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:17:29.850151       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 1m20s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:17:29.850641       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:18:49.850348       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:18:49.850463       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:18:49.850916       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:18:49.922612       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:18:49.922699       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:18:49.922797       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:18:49.965685       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:18:49.965719       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:18:50.109017       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:18:50.109075       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:18:50.109114       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 2m40s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:18:50.109185       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:21:30.109360       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:21:30.109431       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:21:30.109752       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:21:30.445597       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:21:30.445680       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:21:30.445720       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:21:30.502630       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:21:30.502655       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:21:30.632318       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:21:30.632374       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:21:30.632768       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5m0s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:21:30.633164       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:26:30.633379       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:26:30.633465       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:26:30.634004       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:26:30.882865       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:26:30.882891       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:26:30.882910       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:26:31.009050       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:26:31.009071       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:26:31.150325       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:26:31.150413       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:26:31.150463       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5m0s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:26:31.150946       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:31:31.151130       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:31:31.151221       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:31:31.151709       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:31:31.222898       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:31:31.222937       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:31:31.222956       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:31:31.267412       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:31:31.267505       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:31:31.573618       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:31:31.573674       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:31:31.573715       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5m0s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:31:31.573822       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:36:31.574023       1 service_controller.go:301] Ensuring LB for service default/nginx-server
I0502 02:36:31.574111       1 azure_loadbalancer.go:438] reconcileLoadBalancer(default/nginx-server) - wantLb(false): started
I0502 02:36:31.574825       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Normal' reason: 'EnsuringLoadBalancer' Ensuring load balancer
I0502 02:36:31.789324       1 azure_backoff.go:197] LoadBalancerClient.List(andy-k8s197) - backoff: success
I0502 02:36:31.789351       1 azure_loadbalancer.go:444] reconcileLoadBalancer(default/nginx-server): lb(andy-k8s197-internal) wantLb(false) resolved load balancer name
I0502 02:36:31.789378       1 azure_loadbalancer.go:800] ensure(default/nginx-server): lb(andy-k8s197-internal) finished
I0502 02:36:31.916911       1 azure_backoff.go:252] PublicIPAddressesClient.List(andy-k8s197) - backoff: success
I0502 02:36:31.916927       1 azure_loadbalancer.go:1177] ensure(default/nginx-server): pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:36:32.126151       1 azure_backoff.go:402] backoff: failure, will retry, HTTP response=400, err=network.PublicIPAddressesClient#Delete: Failure responding to request: StatusCode=400 -- Original Error: autorest/azure: Service returned an error. Status=400 Code="PublicIPAddressCannotBeDeleted" Message="Public IP address /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/publicIPAddresses/andy-k8s197-a28853f7f4dae11e8a78c000d3a01687 can not be deleted since it is still allocated to resource /subscriptions/4be8920b-2978-43d7-ab14-04d8549c1d05/resourceGroups/andy-k8s197/providers/Microsoft.Network/loadBalancers/andy-k8s197/frontendIPConfigurations/a28853f7f4dae11e8a78c000d3a01687." Details=[]
I0502 02:36:32.126197       1 azure_loadbalancer.go:1182] ensure(default/nginx-server) abort backoff: pip(andy-k8s197-a28853f7f4dae11e8a78c000d3a01687) - deleting
E0502 02:36:32.126237       1 service_controller.go:776] Failed to process service default/nginx-server. Retrying in 5m0s: failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
I0502 02:36:32.126268       1 event.go:218] Event(v1.ObjectReference{Kind:"Service", Namespace:"default", Name:"nginx-server", UID:"527f0e45-4dae-11e8-a78c-000d3a016870", APIVersion:"v1", ResourceVersion:"548398", FieldPath:""}): type: 'Warning' reason: 'CreatingLoadBalancerFailed' Error creating load balancer (will retry): failed to ensure load balancer for service default/nginx-server: timed out waiting for the condition
